{
	"jobConfig": {
		"name": "sf_de_framework_spark_glue_script",
		"description": "",
		"role": "arn:aws:iam::440897580919:role/ut_de_framework_glue_role",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 30,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "sf_de_framework_spark_glue_script.py",
		"scriptLocation": "s3://aws-glue-assets-440897580919-us-west-2/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--conf spark.driver.userClassPathFirst",
				"value": "true",
				"existing": false
			},
			{
				"key": "--conf spark.executor.userClassPathFirst",
				"value": "true",
				"existing": false
			},
			{
				"key": "--python-modules-installer-option",
				"value": "--only-binary=:all: --no-cache-dir",
				"existing": false
			},
			{
				"key": "--user-jars-first",
				"value": "true",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-23T15:34:47.001Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-440897580919-us-west-2/temporary/",
		"additionalPythonModules": "croniter==1.4.1,pytz==2024.2,snowflake-connector-python==3.15.0,s3://awsus-rs-df-01/libs/paramiko-3.5.0-py3-none-any.whl,openpyxl,azure-storage-blob==12.22.0",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"dependentPath": "s3://sf-de-framework-bucket/jdbc_drivers/ngdbc-2.25.13.jar,s3://sf-de-framework-bucket/glue_jars/spark-salesforce_2.12-1.1.4.jar,s3://sf-de-framework-bucket/glue_jars/salesforce-wave-api-1.0.10.jar,s3://sf-de-framework-bucket/glue_jars/force-wsc-40.0.0.jar,s3://sf-de-framework-bucket/glue_jars/force-partner-api-40.0.0.jar,s3://sf-de-framework-bucket/glue_jars/jackson-dataformat-xml-2.4.4.jar,s3://sf-de-framework-bucket/glue_jars/woodstox-core-asl-4.4.0.jar,s3://sf-de-framework-bucket/glue_jars/stax2-api-3.1.4.jar,s3://sf-de-framework-bucket/glue_jars/commons-logging-1.2.jar,s3://sf-de-framework-bucket/glue_jars/commons-codec-1.11.jar,s3://sf-de-framework-bucket/glue_jars/httpclient-4.5.13.jar,s3://sf-de-framework-bucket/glue_jars/httpcore-4.4.13.jar,s3://sf-de-framework-bucket/glue_jars/ojdbc8-23.9.0.25.07.jar",
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-440897580919-us-west-2/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "# -*- coding: utf-8 -*-\r\n\"\"\"\r\nAWS Glue ETL Script for Snowflake Data Framework\r\nThis script handles ETL pipelines with JDBC source connections.\r\nConfiguration tables used for file options:\r\n  - CONFIG.PROPERTIES (formerly TYPECONFIG)\r\n  - CONFIG.PIPELINEPROPERTIES (formerly CUSTOMCONFIG)\r\n  - CONFIG.SYSTEMPROPERTIES (new) for static values / runtime tuning\r\n\"\"\"\r\n\r\nfrom awsglue.transforms import *\r\nfrom awsglue.context import GlueContext\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql import Row, SparkSession, functions as F\r\nfrom pyspark.sql.types import *\r\nfrom pyspark.sql.window import Window\r\nfrom pyspark.sql.utils import AnalysisException\r\nfrom pyspark.sql.functions import col, expr, lit\r\nfrom pyspark.sql import types as T\r\nfrom pyspark.sql.types import (\r\n    StructType,\r\n    StructField,\r\n    IntegerType,\r\n    DoubleType,\r\n    BooleanType,\r\n    StringType,\r\n    TimestampType,\r\n    LongType,\r\n)\r\nfrom datetime import datetime, timezone, timedelta\r\nfrom croniter import croniter\r\nimport snowflake.connector\r\nimport logging\r\nimport boto3\r\nimport sys\r\nimport traceback\r\nimport json\r\nimport pytz\r\nimport uuid\r\nimport time\r\nimport os\r\nimport re\r\nimport paramiko\r\nimport pandas as pd\r\nimport io\r\nfrom io import BytesIO\r\nimport requests  # <-- needed for API\r\nimport openpyxl  # <-- Need for XLSX\r\nfrom requests.auth import HTTPBasicAuth  # <-- needed for API\r\nfrom typing import Optional, Tuple, Dict, Any\r\nimport pyarrow.parquet as pq # use pyarrow to read parquet in-memory\r\n\r\n# --- CHANGED: make Azure Blob dependency optional ---\r\ntry:\r\n    from azure.storage.blob import BlobServiceClient  # <-- for Azure Blob support\r\nexcept ImportError:\r\n    BlobServiceClient = None\r\n# ----------------------------------------------------\r\n\r\nimport fnmatch  # <-- for wildcard matching of blob names\r\n\r\n# ===== INITIALIZATION SECTION =====\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\n\r\n# Defaults that preserve current behavior if SYSTEMPROPERTIES is missing\r\nDEFAULTS = {\r\n    \"SECRETS_REGION\": \"us-west-2\",\r\n    # >>> CHANGED <<< (placeholder; real values computed after SYSTEMPROPERTIES)\r\n    \"MASTERLOG_TABLE\": None,\r\n    \"LOG_TABLE\": None,\r\n    \"CRON_TABLE\": None,\r\n    \"PIPELINE_SUCCESS_ID\": \"1\",\r\n    \"MASTER_SUCCESS_ID\": \"3\",\r\n    \"MASTER_FAILURE_ID\": \"4\",\r\n    \"STAGE_TABLE_SUFFIX\": \"_STAGE\",\r\n    \"DEFAULT_MAX_RETRIES\": \"3\",\r\n    \"DEFAULT_RETRY_DELAY_SECONDS\": \"60\",\r\n    # >>> CHANGED <<< set where config tables live (hard default to SF_DE_FRAMEWORK.CONFIG)\r\n    \"CONFIG_DATABASE\": \"SF_DE_FRAMEWORK\",\r\n    \"CONFIG_SCHEMA\": \"CONFIG\",\r\n    # Global Alerts\r\n    \"sourceSystemName\": \"DF_Snowflake\",\r\n    \"GlobalAlerts_API_URL\": \"https://utglobalalertsapi.azurewebsites.net/api/\",\r\n    \"DEFAULT_TIMEOUT\": \"30\",\r\n    \"BusinessAlertEmail\": \"bharathirajk@unitedtechno.com\",\r\n}\r\n\r\n# Will be updated after loading SYSTEMPROPERTIES\r\nDEFAULT_AWS_REGION = DEFAULTS[\"SECRETS_REGION\"]\r\n\r\n# ===== LOGGING SETUP SECTION =====\r\ndef get_standard_logger(name=\"glueLogger\", level=logging.INFO):\r\n    logger = logging.getLogger(name)\r\n    logger.setLevel(level)\r\n    if not logger.handlers:\r\n        handler = logging.StreamHandler(sys.stdout)\r\n        formatter = logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\")\r\n        handler.setFormatter(formatter)\r\n        logger.addHandler(handler)\r\n    return logger\r\n\r\n\r\nlogger = get_standard_logger()\r\nsecret_cache = {}\r\n\r\n\r\n# ===================== NEW: VALIDATION HELPERS (collect all errors) =====================\r\nclass NonRetryableError(Exception):\r\n    pass\r\n\r\n\r\ndef _is_blank(v):\r\n    return v is None or (isinstance(v, str) and v.strip() == \"\")\r\n\r\n\r\ndef _add_error(errors, msg):\r\n    if msg not in errors:\r\n        errors.append(msg)\r\n\r\n\r\ndef _require_fields(row_dict, checks, errors):\r\n    for field, msg in checks:\r\n        if field not in row_dict or _is_blank(row_dict[field]):\r\n            _add_error(errors, msg)\r\n\r\n\r\ndef _require_active(row_dict, msg_inactive, errors):\r\n    val = row_dict.get(\"ACTIVE\")\r\n    if val is None:\r\n        _add_error(errors, msg_inactive)\r\n        return\r\n    sval = str(val).strip().lower()\r\n    if not (val is True or sval in [\"true\", \"1\", \"t\", \"y\", \"yes\"]):\r\n        _add_error(errors, msg_inactive)\r\n\r\n\r\n# >>> CHANGED <<< pass in cfg() to pull from the correct DB/Schema\r\ndef validate_all_required(spark, sf_options, pid, cfg):\r\n    \"\"\"\r\n    Validate required columns across CONFIG tables for the given pipeline.\r\n    Returns a list of error messages (empty list if OK).\r\n    \"\"\"\r\n    errors = []\r\n\r\n    pipeline_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"PIPELINE\"))\r\n        .load()\r\n    )\r\n    source_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"SOURCE\"))\r\n        .load()\r\n    )\r\n    destination_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"DESTINATION\"))\r\n        .load()\r\n    )\r\n    lookup_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"LOOKUP\"))\r\n        .load()\r\n    )\r\n    secret_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"SECRET\"))\r\n        .load()\r\n    )\r\n    src_col_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"SOURCECOLUMN\"))\r\n        .load()\r\n    )\r\n    dest_col_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"DESTINATIONCOLUMN\"))\r\n        .load()\r\n    )\r\n    col_map_df = (\r\n        spark.read.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", cfg(\"COLUMNMAPPING\"))\r\n        .load()\r\n    )\r\n\r\n    # PIPELINE\r\n    p_row = pipeline_df.filter(col(\"PIPELINEID\") == pid).first()\r\n    if not p_row:\r\n        _add_error(\r\n            errors,\r\n            \"PIPELINEID in PIPELINE table is mandatory; empty values are not permitted.\",\r\n        )\r\n        return errors\r\n    p = p_row.asDict()\r\n    _require_fields(\r\n        p,\r\n        [\r\n            (\r\n                \"PIPELINENAME\",\r\n                \"PIPELINENAME in PIPELINE table is mandatory; empty values are not permitted.\",\r\n            ),\r\n            (\r\n                \"SOURCEID\",\r\n                \"SOURCEID in PIPELINE table is mandatory; empty values are not permitted.\",\r\n            ),\r\n            (\r\n                \"DESTINATIONID\",\r\n                \"DESTINATIONID in PIPELINE table is mandatory; empty values are not permitted.\",\r\n            ),\r\n            (\r\n                \"LOADTYPEID\",\r\n                \"LOADTYPEID in PIPELINE table is mandatory; empty values are not permitted.\",\r\n            ),\r\n        ],\r\n        errors,\r\n    )\r\n\r\n    # SOURCE\r\n    s_row = source_df.filter(col(\"SOURCEID\") == p.get(\"SOURCEID\")).first()\r\n    if not s_row:\r\n        _add_error(\r\n            errors,\r\n            \"SOURCEID in SOURCE table is mandatory; empty values are not permitted.\",\r\n        )\r\n        s = {}\r\n    else:\r\n        s = s_row.asDict()\r\n        _require_fields(\r\n            s,\r\n            [\r\n                (\r\n                    \"SOURCENAME\",\r\n                    \"SOURCENAME in SOURCE table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"SOURCEPATH\",\r\n                    \"SOURCEPATH in SOURCE table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"SOURCETYPEID\",\r\n                    \"SOURCETYPEID in SOURCE table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"FORMATTYPEID\",\r\n                    \"FORMATTYPEID in SOURCE table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"SECRETID\",\r\n                    \"SECRETID in SOURCE table is mandatory; empty values are not permitted.\",\r\n                ),\r\n            ],\r\n            errors,\r\n        )\r\n        _require_active(\r\n            s,\r\n            \"The values in the ACTIVE column of the SOURCE table are not set to active.\",\r\n            errors,\r\n        )\r\n\r\n    # DESTINATION\r\n    d_row = destination_df.filter(col(\"DESTINATIONID\") == p.get(\"DESTINATIONID\")).first()\r\n    if not d_row:\r\n        _add_error(\r\n            errors,\r\n            \"DESTINATIONID in DESTINATION table is mandatory; empty values are not permitted.\",\r\n        )\r\n        d = {}\r\n    else:\r\n        d = d_row.asDict()\r\n        _require_fields(\r\n            d,\r\n            [\r\n                (\r\n                    \"DESTINATIONNAME\",\r\n                    \"DESTINATIONNAME in DESTINATION table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"DESTINATIONPATH\",\r\n                    \"DESTINATIONPATH in DESTINATION table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"DESTINATIONTYPEID\",\r\n                    \"DESTINATIONTYPEID in DESTINATION table is mandatory; empty values are not permitted.\",\r\n                ),\r\n                (\r\n                    \"FORMATTYPEID\",\r\n                    \"FORMATTYPEID in DESTINATION table is mandatory; empty values are not permitted.\",\r\n                ),\r\n            ],\r\n            errors,\r\n        )\r\n        _require_active(\r\n            d,\r\n            \"The values in the ACTIVE column of the DESTINATION table are not set to active.\",\r\n            errors,\r\n        )\r\n\r\n    # LOOKUPs referenced\r\n    needed_lookup_ids = []\r\n    for key in [\"LOADTYPEID\", \"LAYERID\"]:\r\n        if p.get(key) is not None:\r\n            needed_lookup_ids.append(int(p[key]))\r\n    for key in [\"SOURCETYPEID\", \"FORMATTYPEID\"]:\r\n        if s.get(key) is not None:\r\n            needed_lookup_ids.append(int(s[key]))\r\n    if d.get(\"FORMATTYPEID\") is not None:\r\n        needed_lookup_ids.append(int(d[\"FORMATTYPEID\"]))\r\n\r\n    if needed_lookup_ids:\r\n        lk_df = lookup_df.filter(col(\"LOOKUPID\").isin(needed_lookup_ids))\r\n        for r in lk_df.collect():\r\n            rd = r.asDict()\r\n            _require_fields(\r\n                rd,\r\n                [\r\n                    (\r\n                        \"LOOKUPID\",\r\n                        \"LOOKUPID in LOOKUP table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                    (\r\n                        \"LOOKUPNAME\",\r\n                        \"LOOKUPNAME in LOOKUP table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                    (\r\n                        \"LOOKUPVALUE\",\r\n                        \"LOOKUPVALUE in LOOKUP table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                ],\r\n                errors,\r\n            )\r\n            _require_active(\r\n                rd,\r\n                \"The values in the ACTIVE column of the LOOKUP table are not set to active.\",\r\n                errors,\r\n            )\r\n\r\n    # SECRETs referenced\r\n    secret_ids = [s.get(\"SECRETID\"), d.get(\"SECRETID\")]\r\n    secret_ids = [x for x in secret_ids if x is not None]\r\n    if secret_ids:\r\n        sec_df = secret_df.filter(col(\"SECRETID\").isin(secret_ids))\r\n        for r in sec_df.collect():\r\n            rd = r.asDict()\r\n            _require_fields(\r\n                rd,\r\n                [\r\n                    (\r\n                        \"SECRETNAME\",\r\n                        \"SECRETNAME in Secret table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                    (\r\n                        \"SECRETKEY\",\r\n                        \"SECRETKEY in Secret table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                ],\r\n                errors,\r\n            )\r\n            _require_active(\r\n                rd,\r\n                \"The values in the ACTIVE column of the Secret table are not set to active.\",\r\n                errors,\r\n            )\r\n\r\n    # COLUMNMAPPING and columns\r\n    dcols = (\r\n        dest_col_df.filter(col(\"DESTINATIONID\") == p.get(\"DESTINATIONID\"))\r\n        .select(\"DESTINATIONCOLUMNID\")\r\n        .distinct()\r\n    )\r\n    dcol_ids = [row[\"DESTINATIONCOLUMNID\"] for row in dcols.collect()]\r\n    if dcol_ids:\r\n        cm_df = col_map_df.filter(col(\"DESTINATIONCOLUMNID\").isin(dcol_ids))\r\n        for r in cm_df.collect():\r\n            rd = r.asDict()\r\n            _require_fields(\r\n                rd,\r\n                [\r\n                    (\r\n                        \"SOURCECOLUMNID\",\r\n                        \"SOURCECOLUMNID in COLUMNMAPPING table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                    (\r\n                        \"DESTINATIONCOLUMNID\",\r\n                        \"DESTINATIONCOLUMNID in COLUMNMAPPING table is mandatory; empty values are not permitted.\",\r\n                    ),\r\n                ],\r\n                errors,\r\n            )\r\n            _require_active(\r\n                rd,\r\n                \"The values in the ACTIVE column of the COLUMNMAPPING table are not set to active.\",\r\n                errors,\r\n            )\r\n\r\n        src_ids = [\r\n            row[\"SOURCECOLUMNID\"]\r\n            for row in cm_df.select(\"SOURCECOLUMNID\").distinct().collect()\r\n        ]\r\n        if src_ids:\r\n            sc_sel = src_col_df.filter(col(\"SOURCECOLUMNID\").isin(src_ids))\r\n            for r in sc_sel.collect():\r\n                rd = r.asDict()\r\n                _require_fields(\r\n                    rd,\r\n                    [\r\n                        (\r\n                            \"SOURCECOLUMNNAME\",\r\n                            \"SOURCECOLUMNNAME in SOURCECOLUMN table is mandatory; empty values are not permitted.\",\r\n                        ),\r\n                        (\r\n                            \"SOURCECOLUMNSEQUENCE\",\r\n                            \"SOURCECOLUMNSEQUENCE in SOURCECOLUMN table is mandatory; empty values are not permitted.\",\r\n                        ),\r\n                        (\r\n                            \"SOURCECOLUMNDATATYPE\",\r\n                            \"SOURCECOLUMNDATATYPE in SOURCECOLUMN table is mandatory; empty values are not permitted.\",\r\n                        ),\r\n                    ],\r\n                    errors,\r\n                )\r\n                _require_active(\r\n                    rd,\r\n                    \"The values in the ACTIVE column of the SOURCECOLUMN table are not set to active.\",\r\n                    errors,\r\n                )\r\n\r\n        if dcol_ids:\r\n            dc_sel = dest_col_df.filter(col(\"DESTINATIONCOLUMNID\").isin(dcol_ids))\r\n            for r in dc_sel.collect():\r\n                rd = r.asDict()\r\n                _require_fields(\r\n                    rd,\r\n                    [\r\n                        (\r\n                            \"DESTINATIONCOLUMNNAME\",\r\n                            \"DESTINATIONCOLUMNNAME in DESTINATIONCOLUMN table is mandatory; empty values are not permitted.\",\r\n                        ),\r\n                        (\r\n                            \"DESTINATIONCOLUMNSEQUENCE\",\r\n                            \"DESTINATIONCOLUMNSEQUENCE in DESTINATIONCOLUMN table is mandatory; empty values are not permitted.\",\r\n                        ),\r\n                        (\r\n                            \"DESTINATIONCOLUMNDATATYPE\",\r\n                            \"DESTINATIONCOLUMNDATATYPE in DESTINATIONCOLUMN table is mandatory; empty values are not permitted.\",\r\n                        ),\r\n                    ],\r\n                    errors,\r\n                )\r\n                _require_active(\r\n                    rd,\r\n                    \"The values in the ACTIVE column of the DESTINATIONCOLUMN table are not set to active.\",\r\n                    errors,\r\n                )\r\n\r\n    return errors\r\n\r\n\r\n# ===================== END NEW HELPERS =====================\r\n\r\n# ===== RETRY MECHANISM SECTION =====\r\ndef spark_snowflake_retry(func, max_retries=3, base_wait=5, backoff=2, action_desc=\"\"):\r\n    attempt = 0\r\n    wait_time = base_wait\r\n    last_exception = None\r\n    while attempt < max_retries:\r\n        try:\r\n            return func()\r\n        except Exception as e:\r\n            last_exception = e\r\n            attempt += 1\r\n            logger.error(f\"Attempt {attempt} for {action_desc} failed: {str(e)}\")\r\n            if attempt < max_retries:\r\n                logger.info(f\"Retrying {action_desc} in {wait_time} seconds...\")\r\n                time.sleep(wait_time)\r\n                wait_time *= backoff\r\n            else:\r\n                logger.error(\r\n                    f\"All {max_retries} retries failed for {action_desc}. Raising error.\"\r\n                )\r\n    raise last_exception\r\n\r\n\r\n# ===== AWS SECRETS MANAGER SECTION =====\r\ndef get_secret_cached(secret_name, region_name=None):\r\n    \"\"\"Retrieve secrets from AWS Secrets Manager with caching\"\"\"\r\n    global DEFAULT_AWS_REGION\r\n    region = region_name or DEFAULT_AWS_REGION\r\n\r\n    cache_key = f\"{region}:{secret_name}\"\r\n    if cache_key in secret_cache:\r\n        logger.info(\r\n            f\"[CACHE HIT] Secret '{secret_name}' ({region}) retrieved from cache.\"\r\n        )\r\n        return secret_cache[cache_key]\r\n\r\n    client = boto3.client(\"secretsmanager\", region_name=region)\r\n\r\n    try:\r\n        response = client.get_secret_value(SecretId=secret_name)\r\n        secret = json.loads(response[\"SecretString\"])\r\n        secret_cache[cache_key] = secret\r\n        logger.info(\r\n            f\"[SUCCESS] Secret '{secret_name}' fetched from AWS Secrets Manager in {region}.\"\r\n        )\r\n        return secret\r\n\r\n    except client.exceptions.ResourceNotFoundException:\r\n        logger.error(\r\n            f\"[ERROR] Secret '{secret_name}' not found in Secrets Manager (region {region}).\"\r\n        )\r\n        raise\r\n    except client.exceptions.ClientError as e:\r\n        logger.error(\r\n            f\"[ERROR] ClientError while fetching secret '{secret_name}' (region {region}): {str(e)}\"\r\n        )\r\n        raise\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"[ERROR] Unexpected error while fetching secret '{secret_name}' (region {region}): {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n\r\n# ===== SNOWFLAKE CONNECTION SECTION =====\r\ndef create_snowflake_options(secret):\r\n    required_keys = [\r\n        \"account\",\r\n        \"user\",\r\n        \"password\",\r\n        \"database\",\r\n        \"schema\",\r\n        \"warehouse\",\r\n        \"role\",\r\n    ]\r\n    missing_keys = [k for k in required_keys if k not in secret or not secret[k]]\r\n    if missing_keys:\r\n        logger.error(\r\n            f\"[ERROR] Missing required Snowflake credential keys: {missing_keys}\"\r\n        )\r\n        raise ValueError(f\"Missing required keys in Snowflake secret: {missing_keys}\")\r\n\r\n    options = {\r\n        \"sfURL\": f\"{secret['account']}.snowflakecomputing.com\",\r\n        \"sfUser\": secret[\"user\"],\r\n        \"sfPassword\": secret[\"password\"],\r\n        \"sfDatabase\": secret[\"database\"],\r\n        \"sfSchema\": secret[\"schema\"],\r\n        \"sfWarehouse\": secret[\"warehouse\"],\r\n        \"sfRole\": secret[\"role\"],\r\n    }\r\n    logger.info(\r\n        f\"[SUCCESS] Snowflake options successfully created for account '{secret['account']}'.\"\r\n    )\r\n    return options\r\n\r\n\r\n# ===== SNOWFLAKE SQL EXECUTION SECTION =====\r\ndef execute_snowflake_sql(secret, sql: str, fetch_result: bool = False):\r\n    attempt = 0\r\n    max_retries = 3\r\n    wait_time = 5\r\n    backoff = 2\r\n    last_exception = None\r\n\r\n    while attempt < max_retries:\r\n        conn, cursor = None, None\r\n        try:\r\n            conn = snowflake.connector.connect(\r\n                user=secret[\"user\"],\r\n                password=secret[\"password\"],\r\n                account=secret[\"account\"],\r\n                warehouse=secret.get(\"warehouse\"),\r\n                database=secret.get(\"database\"),\r\n                schema=secret.get(\"schema\"),\r\n                role=secret.get(\"role\"),\r\n            )\r\n            cursor = conn.cursor()\r\n            logger.debug(f\"Executing SQL (Attempt {attempt + 1}):\\n{sql.strip()}\")\r\n            cursor.execute(sql)\r\n            if fetch_result:\r\n                result = cursor.fetchall()\r\n                logger.debug(f\"Query returned {len(result)} rows.\")\r\n                return result\r\n            return True\r\n        except Exception as e:\r\n            last_exception = e\r\n            attempt += 1\r\n            logger.error(f\"Snowflake SQL attempt {attempt} failed: {str(e)}\")\r\n            if attempt < max_retries:\r\n                logger.info(f\"Retrying Snowflake SQL in {wait_time} seconds...\")\r\n                time.sleep(wait_time)\r\n                wait_time *= backoff\r\n            else:\r\n                logger.error(\r\n                    f\"All {max_retries} connection attempts failed. Raising error.\"\r\n                )\r\n        finally:\r\n            try:\r\n                if cursor:\r\n                    cursor.close()\r\n                if conn:\r\n                    conn.close()\r\n            except Exception:\r\n                pass\r\n    raise last_exception\r\n\r\n\r\n# ===== TABLE EXISTENCE CHECK SECTION =====\r\ndef table_exists_in_snowflake(secret, table_name):\r\n    try:\r\n        parts = table_name.upper().split(\".\")\r\n        if len(parts) == 3:\r\n            db_name, schema_name, tbl_name = parts\r\n        elif len(parts) == 2:\r\n            db_name = secret.get(\"database\", \"\").upper()\r\n            schema_name, tbl_name = parts\r\n        else:\r\n            db_name = secret.get(\"database\", \"\").upper()\r\n            schema_name = secret.get(\"schema\", \"\").upper()\r\n            tbl_name = parts[0]\r\n        sql = f\"\"\"\r\n        SELECT COUNT(1) AS CNT \r\n        FROM INFORMATION_SCHEMA.TABLES \r\n        WHERE TABLE_CATALOG = '{db_name}' \r\n          AND TABLE_SCHEMA = '{schema_name}' \r\n          AND TABLE_NAME = '{tbl_name}'\r\n        \"\"\"\r\n        results = execute_snowflake_sql(secret, sql, fetch_result=True)\r\n        return results[0][0] > 0 if results else False\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Failed to check if table exists: {table_name}. Error: {str(e)}\"\r\n        )\r\n        return False\r\n\r\n\r\n# ===== STATUS MESSAGE HELPERS =====\r\ndef get_status_message_name_by_id(status_df, statusmessage_id):\r\n    if status_df.rdd.isEmpty():\r\n        raise Exception(\r\n            \"STATUSMESSAGE table is empty. Please populate it before proceeding.\"\r\n        )\r\n    filtered_df = status_df.filter(col(\"STATUSMESSAGEID\") == statusmessage_id)\r\n    if filtered_df.rdd.isEmpty():\r\n        raise Exception(\r\n            f\"STATUSMESSAGEID '{statusmessage_id}' not found in STATUSMESSAGE table.\"\r\n        )\r\n    return filtered_df.select(\"STATUSMESSAGENAME\").first()[\"STATUSMESSAGENAME\"]\r\n\r\n\r\ndef get_status_by_name(status_df, name, default_id=4):\r\n    row = (\r\n        status_df.filter(F.upper(col(\"STATUSMESSAGENAME\")) == name.upper())\r\n        .select(\"STATUSMESSAGENAME\", \"STATUSMESSAGEID\")\r\n        .select(\"STATUSMESSAGEID\", \"STATUSMESSAGENAME\")\r\n        .first()\r\n    )\r\n    if row:\r\n        return int(row[\"STATUSMESSAGEID\"]), row[\"STATUSMESSAGENAME\"]\r\n    return int(default_id), get_status_message_name_by_id(status_df, default_id)\r\n\r\n\r\n# ===== NEW: fetch STATUSMESSAGECONTENT by name (for MASTERLOG.MESSAGE) =====\r\ndef get_statusmessage_content_by_name(status_df, name):\r\n    row = (\r\n        status_df.filter(F.upper(col(\"STATUSMESSAGENAME\")) == name.upper())\r\n        .select(\"STATUSMESSAGECONTENT\")\r\n        .first()\r\n    )\r\n    return (\r\n        row[\"STATUSMESSAGECONTENT\"]\r\n        if row and row[\"STATUSMESSAGECONTENT\"] is not None\r\n        else None\r\n    )\r\n\r\n\r\ndef classify_error(status_df, err_msg: str):\r\n    msg = (err_msg or \"\").lower()\r\n    if re.search(r\"login failed\", msg):\r\n        return get_status_by_name(status_df, \"PIPELINE_FAILURE_AUTH_FAILED\")\r\n    if re.search(r\"(invalid object name|does not exist|table not found)\", msg):\r\n        return get_status_by_name(status_df, \"PIPELINE_FAILURE_TABLE_NOT_FOUND\")\r\n    return get_status_by_name(status_df, \"MASTER_FAILURE\")  # (= id 4)\r\n\r\n\r\n# ===== MINIMAL: PIPELINE RETRY CONFIG (returns defaults) =====\r\ndef get_retry_config_for_pipeline(pipeline_df, pid, default_retries=3, default_delay=60):\r\n    return int(default_retries), int(default_delay)\r\n\r\n\r\n# ===== MASTER LOG MANAGEMENT =====\r\ndef create_master_log(spark, sf_options, glue_run_id, total_pipelines, table_name):\r\n    try:\r\n        start_dt = datetime.now(timezone.utc)\r\n        logger.info(\r\n            f\"Creating MASTERLOG entry for Glue RUNID={glue_run_id} at {start_dt} with {total_pipelines} pipelines\"\r\n        )\r\n        master_log_id = str(uuid.uuid4())\r\n        schema = StructType(\r\n            [\r\n                StructField(\"MASTERLOGID\", StringType(), True),\r\n                StructField(\"RUNID\", StringType(), True),\r\n                StructField(\"STARTDATETIME\", TimestampType(), True),\r\n                StructField(\"ENDDATETIME\", TimestampType(), True),\r\n                StructField(\"STATUS\", StringType(), True),\r\n                StructField(\"MESSAGE\", StringType(), True),\r\n                StructField(\"TOTALPIPELINECOUNT\", IntegerType(), True),\r\n            ]\r\n        )\r\n        data = [(master_log_id, glue_run_id, start_dt, None, None, None, total_pipelines)]\r\n        insert_df = spark.createDataFrame(data, schema=schema)\r\n\r\n        spark_snowflake_retry(\r\n            lambda: insert_df.write.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"dbtable\", table_name)\r\n            .mode(\"append\")\r\n            .save(),\r\n            action_desc=\"master_log insert\",\r\n        )\r\n        logger.info(f\"MASTERLOG entry created successfully → ID: {master_log_id}\")\r\n        return master_log_id\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Error creating MASTERLOG entry: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n\r\n# ===== SYSTEM PROPERTIES LOADER =====\r\ndef load_system_properties(spark, sf_options, fqn_table):\r\n    \"\"\"\r\n    Loads CONFIG.SYSTEMPROPERTIES from the given fully-qualified table.\r\n    Returns a dict with UPPERCASE keys.\r\n    \"\"\"\r\n    try:\r\n        df = (\r\n            spark.read.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"dbtable\", fqn_table)\r\n            .load()\r\n        )\r\n        props = {}\r\n        if \"ACTIVE\" in df.columns:\r\n            df = df.filter(col(\"ACTIVE\") == True)\r\n        for r in df.select(\"PROPERTYKEY\", \"PROPERTYVALUE\").collect():\r\n            if r[\"PROPERTYKEY\"]:\r\n                props[str(r[\"PROPERTYKEY\"]).upper()] = (\r\n                    str(r[\"PROPERTYVALUE\"]) if r[\"PROPERTYVALUE\"] is not None else None\r\n                )\r\n        logger.info(f\"Loaded {len(props)} system properties from {fqn_table}\")\r\n        return props\r\n    except Exception as e:\r\n        logger.warning(\r\n            f\"Unable to load SYSTEMPROPERTIES ({fqn_table}), using defaults. Error: {e}\"\r\n        )\r\n        return {}\r\n\r\n\r\ndef get_prop(props, key, default_value):\r\n    return (props.get(key.upper()) if props else None) or default_value\r\n\r\n\r\n# ===== METADATA LOADING SECTION =====\r\ndef load_metadata(spark, sf_options, pid, cfg):\r\n    logger.info(f\"Starting metadata load for pipeline ID: {pid}\")\r\n\r\n    def load_df(table_fqn):\r\n        try:\r\n            logger.info(f\"Loading table: {table_fqn}\")\r\n            return spark_snowflake_retry(\r\n                lambda: spark.read.format(\"snowflake\")\r\n                .options(**sf_options)\r\n                .option(\"dbtable\", table_fqn)\r\n                .load(),\r\n                action_desc=f\"read {table_fqn}\",\r\n            )\r\n        except Exception as e:\r\n            logger.error(\r\n                f\"Failed to load table: {table_fqn}. Error: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n            )\r\n            raise\r\n\r\n    pipeline_df = load_df(cfg(\"PIPELINE\"))\r\n    lookup_df = load_df(cfg(\"LOOKUP\"))\r\n    source_df = load_df(cfg(\"SOURCE\"))\r\n    destination_df = load_df(cfg(\"DESTINATION\"))\r\n    source_col_df = load_df(cfg(\"SOURCECOLUMN\"))\r\n    dest_col_df = load_df(cfg(\"DESTINATIONCOLUMN\"))\r\n    col_mapping_df = load_df(cfg(\"COLUMNMAPPING\"))\r\n    pipeline_props_df = load_df(cfg(\"PIPELINEPROPERTIES\"))  # was CUSTOMCONFIG\r\n    properties_df = load_df(cfg(\"PROPERTIES\"))  # was TYPECONFIG\r\n    secret_df = load_df(cfg(\"SECRET\"))\r\n\r\n    logger.info(\"All tables loaded successfully from Snowflake (config schema).\")\r\n\r\n    p_df = pipeline_df.filter(col(\"PIPELINEID\") == pid)\r\n    if p_df.count() == 0:\r\n        logger.error(f\"No PIPELINE row found for PIPELINEID={pid}\")\r\n        raise ValueError(f\"No PIPELINE row found for PIPELINEID={pid}\")\r\n    p = p_df.first().asDict()\r\n    logger.info(f\"[Debug] PIPELINE row → LOADTYPEID={p.get('LOADTYPEID')}\")\r\n\r\n    load_type_df = lookup_df.filter(\r\n        (col(\"LOOKUPNAME\") == \"LoadType\") & (col(\"LOOKUPID\") == p[\"LOADTYPEID\"])\r\n    )\r\n    if load_type_df.count() == 0:\r\n        logger.error(f\"Invalid LOADTYPEID={p['LOADTYPEID']}\")\r\n        raise ValueError(f\"Invalid LOADTYPEID={p['LOADTYPEID']}\")\r\n    p[\"LOAD_TYPE\"] = load_type_df.first()[\"LOOKUPVALUE\"]\r\n\r\n    layer_df = lookup_df.filter(\r\n        (col(\"LOOKUPNAME\") == \"Layer\") & (col(\"LOOKUPID\") == p[\"LAYERID\"])\r\n    )\r\n    if layer_df.count() == 0:\r\n        logger.error(f\"Invalid LAYERID={p['LAYERID']}\")\r\n        raise ValueError(f\"Invalid LAYERID={p['LAYERID']}\")\r\n    p[\"LAYER\"] = layer_df.first()[\"LOOKUPVALUE\"]\r\n\r\n    d_df = destination_df.filter(col(\"DESTINATIONID\") == p[\"DESTINATIONID\"])\r\n    if d_df.count() == 0:\r\n        logger.error(f\"Invalid DESTINATIONID={p['DESTINATIONID']}\")\r\n        raise ValueError(f\"Invalid DESTINATIONID={p['DESTINATIONID']}\")\r\n    d = d_df.first().asDict()\r\n\r\n    scd_lookup_id = d.get(\"SCDTYPE\", d.get(\"SCDTYPEID\"))\r\n    if scd_lookup_id is None:\r\n        d[\"SCDTYPE\"] = 21\r\n        d[\"SCD_TYPE\"] = \"NONE\"\r\n        logger.info(\r\n            \"DESTINATION SCDTYPE/SCDTYPEID not provided; defaulting to 21 ('NONE').\"\r\n        )\r\n    else:\r\n        scd_df = lookup_df.filter(\r\n            (col(\"LOOKUPNAME\") == \"ScdType\") & (col(\"LOOKUPID\") == lit(int(scd_lookup_id)))\r\n        )\r\n        if scd_df.count() == 0:\r\n            d[\"SCDTYPE\"] = 21\r\n            d[\"SCD_TYPE\"] = \"NONE\"\r\n            logger.warning(\r\n                f\"SCDTYPE value {scd_lookup_id} not found in LOOKUP; defaulted to 21 ('NONE').\"\r\n            )\r\n        else:\r\n            d[\"SCD_TYPE\"] = scd_df.first()[\"LOOKUPVALUE\"]\r\n            d[\"SCDTYPE\"] = int(scd_lookup_id)\r\n\r\n    if \"SECRETID\" in d and d[\"SECRETID\"] is not None:\r\n        dest_secret_df = secret_df.filter(col(\"SECRETID\") == d[\"SECRETID\"])\r\n        if dest_secret_df.count() == 0:\r\n            logger.error(f\"Invalid SECRETID={d['SECRETID']} in destination table\")\r\n            raise ValueError(f\"Invalid SECRETID={d['SECRETID']}\")\r\n        d[\"SECRET_KEY\"] = dest_secret_df.first()[\"SECRETKEY\"]\r\n    else:\r\n        logger.info(\r\n            \"DESTINATION.SECRETID not provided; using global Snowflake credentials for destination writes.\"\r\n        )\r\n        d[\"SECRET_KEY\"] = None\r\n\r\n    s_df = source_df.filter(col(\"SOURCEID\") == p[\"SOURCEID\"])\r\n    if s_df.count() == 0:\r\n        logger.error(f\"Invalid SOURCEID={p['SOURCEID']}\")\r\n        raise ValueError(f\"Invalid SOURCEID={p['SOURCEID']}\")\r\n    s = s_df.first().asDict()\r\n\r\n    source_type_df = lookup_df.filter(\r\n        (col(\"LOOKUPNAME\") == \"SourceType\") & (col(\"LOOKUPID\") == s[\"SOURCETYPEID\"])\r\n    )\r\n    if source_type_df.count() == 0:\r\n        logger.error(\r\n            f\"Invalid SOURCETYPEID={s['SOURCETYPEID']} for SOURCEID={s['SOURCEID']}\"\r\n        )\r\n        raise ValueError(f\"Invalid SOURCETYPEID={s['SOURCETYPEID']}\")\r\n    s[\"SOURCE_TYPE\"] = source_type_df.first()[\"LOOKUPVALUE\"]\r\n\r\n    format_type_df = lookup_df.filter(\r\n        (col(\"LOOKUPNAME\") == \"FormatType\") & (col(\"LOOKUPID\") == s[\"FORMATTYPEID\"])\r\n    )\r\n    if format_type_df.count() == 0:\r\n        logger.error(f\"Invalid FORMATTYPEID={s['FORMATTYPEID']}\")\r\n        raise ValueError(f\"Invalid FORMATTYPEID={s['FORMATTYPEID']}\")\r\n    s[\"FORMAT_TYPE\"] = format_type_df.first()[\"LOOKUPVALUE\"]\r\n\r\n    src_secret_df = secret_df.filter(col(\"SECRETID\") == s[\"SECRETID\"])\r\n    if src_secret_df.count() == 0:\r\n        logger.error(f\"Invalid SECRETID={s['SECRETID']} in source table\")\r\n        raise ValueError(f\"Invalid SECRETID={s['SECRETID']}\")\r\n    s[\"SECRET_KEY\"] = src_secret_df.first()[\"SECRETKEY\"]\r\n\r\n    logger.info(\"Preparing column mappings...\")\r\n    mappings_df = (\r\n        dest_col_df.filter(col(\"DESTINATIONID\") == d[\"DESTINATIONID\"])\r\n        .join(col_mapping_df, [\"DESTINATIONCOLUMNID\"])\r\n        .join(source_col_df, [\"SOURCECOLUMNID\"])\r\n        .select(\r\n            \"SOURCECOLUMNNAME\",\r\n            \"DESTINATIONCOLUMNNAME\",\r\n            \"DESTINATIONCOLUMNDATATYPE\",\r\n            \"DESTINATIONCOLUMNSEQUENCE\",\r\n        )\r\n    )\r\n    mappings = [row.asDict() for row in mappings_df.collect()]\r\n\r\n    logger.info(\"Extracting file format settings...\")\r\n    file_cfg = (\r\n        pipeline_props_df.filter(col(\"PIPELINEID\") == pid)\r\n        .join(properties_df, [\"PROPERTYID\"])\r\n        .select(\"PROPERTYNAME\", \"PROPERTYVALUE\")\r\n    )\r\n    file_settings = {\r\n        row[\"PROPERTYNAME\"].lower(): row[\"PROPERTYVALUE\"] for row in file_cfg.collect()\r\n    }\r\n    required_keys = [\r\n        \"compression\",\r\n        \"skipheader\",\r\n        \"delimiter\",\r\n        \"quotechar\",\r\n        \"escape\",\r\n        \"lineseparator\",\r\n    ]\r\n    for key in required_keys:\r\n        if key not in file_settings:\r\n            logger.warning(\r\n                f\"Missing file setting: {key} for pipeline {pid}. Default may be required.\"\r\n            )\r\n\r\n    logger.info(\"Successfully loaded all metadata.\")\r\n    return {\r\n        \"pipeline\": p,\r\n        \"source\": s,\r\n        \"destination\": d,\r\n        \"mappings\": mappings,\r\n        \"file_settings\": file_settings,\r\n    }\r\n\r\n\r\n# ===== AZURE BLOB HELPER SECTION =====\r\ndef read_from_azureblob_xlsx(spark, md):\r\n    \"\"\"\r\n    Read XLSX files from Azure Blob storage for source_type = 'azureblob'.\r\n\r\n    Expected secret (SECRET_KEY) structure (example):\r\n      {\r\n        \"account_name\": \"yourstorageacct\",\r\n        \"container\": \"yourcontainer\",\r\n        \"connection_string\": \"...\",   # OR\r\n        \"sas_token\": \"...\",           # OR\r\n        \"account_key\": \"...\"\r\n      }\r\n\r\n    SOURCEPATH: treated as a prefix/folder, e.g. \"aero/\".\r\n    file_settings.pattern: wildcard for filenames under that prefix, e.g. \"ANIMALTYPES*.xlsx\"\r\n    \"\"\"\r\n\r\n    # --- NEW: guard when azure-storage-blob is not installed ---\r\n    if BlobServiceClient is None:\r\n        raise NonRetryableError(\r\n            \"Azure Blob support requires the 'azure-storage-blob' package to be \"\r\n            \"installed in this Glue job. Either add that dependency or disable \"\r\n            \"pipelines with SOURCE_TYPE='azureblob'.\"\r\n        )\r\n    # -----------------------------------------------------------\r\n\r\n    fs = {k.lower(): v for k, v in md.get(\"file_settings\", {}).items()}\r\n    pattern = fs.get(\"pattern\") or \"*\"\r\n    skipheader = fs.get(\"skipheader\", \"1\")\r\n\r\n    source = md[\"source\"]\r\n    source_path = source.get(\"SOURCEPATH\") or \"\"\r\n    secret_key = source.get(\"SECRET_KEY\")\r\n\r\n    if not secret_key:\r\n        raise ValueError(\"AzureBlob source requires SECRET_KEY to be populated in metadata.\")\r\n\r\n    az_secret = get_secret_cached(secret_key)\r\n\r\n    connection_string = az_secret.get(\"connection_string\")\r\n    account_name = az_secret.get(\"account_name\")\r\n    container_name = az_secret.get(\"container\")\r\n    sas_token = az_secret.get(\"sas_token\")\r\n    account_key = az_secret.get(\"account_key\")\r\n\r\n    if not container_name:\r\n        raise ValueError(\r\n            \"AzureBlob secret must contain 'container' or 'connection_string' including container info.\"\r\n        )\r\n\r\n    if connection_string:\r\n        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\r\n        container_client = blob_service_client.get_container_client(container_name)\r\n    else:\r\n        if not account_name:\r\n            raise ValueError(\r\n                \"AzureBlob secret must contain 'account_name' when 'connection_string' is not provided.\"\r\n            )\r\n        credential = sas_token or account_key\r\n        if not credential:\r\n            raise ValueError(\r\n                \"AzureBlob secret must contain either 'sas_token', 'account_key', or 'connection_string'.\"\r\n            )\r\n        account_url = f\"https://{account_name}.blob.core.windows.net\"\r\n        blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\r\n        container_client = blob_service_client.get_container_client(container_name)\r\n\r\n    prefix = source_path or \"\"\r\n    if prefix and not prefix.endswith(\"/\"):\r\n        prefix = prefix + \"/\"\r\n\r\n    logger.info(\r\n        f\"Reading from Azure Blob container='{container_name}', prefix='{prefix}', pattern='{pattern}'\"\r\n    )\r\n\r\n    matched_pandas_dfs = []\r\n\r\n    for blob in container_client.list_blobs(name_starts_with=prefix):\r\n        blob_name = blob.name\r\n        rel_name = blob_name[len(prefix) :] if blob_name.startswith(prefix) else blob_name\r\n        if not fnmatch.fnmatch(rel_name, pattern):\r\n            continue\r\n\r\n        logger.info(f\"Reading Azure Blob object: {blob_name}\")\r\n        blob_data = container_client.download_blob(blob_name).readall()\r\n\r\n        header_row = 0 if str(skipheader).strip() != \"0\" else None\r\n        df_pd = pd.read_excel(BytesIO(blob_data), header=header_row)\r\n        matched_pandas_dfs.append(df_pd)\r\n\r\n    if not matched_pandas_dfs:\r\n        raise ValueError(\r\n            f\"No blobs found in container '{container_name}' matching prefix '{prefix}' and pattern '{pattern}'.\"\r\n        )\r\n\r\n    if len(matched_pandas_dfs) == 1:\r\n        combined_pd = matched_pandas_dfs[0]\r\n    else:\r\n        combined_pd = pd.concat(matched_pandas_dfs, ignore_index=True)\r\n\r\n    df = spark.createDataFrame(combined_pd)\r\n    logger.info(f\"AzureBlob read success. Loaded {len(combined_pd.index)} rows.\")\r\n    return df\r\n\r\n#================Flatten JSON Start ===================================\r\ndef _is_struct(col_type):\r\n    return isinstance(col_type, T.StructType)\r\n\r\ndef _is_array(col_type):\r\n    return isinstance(col_type, T.ArrayType)\r\n\r\ndef flatten_structs(df):\r\n    \"\"\"\r\n    Flatten StructType columns WITHOUT prefix.\r\n    Example: config.region -> region\r\n    If duplicate column names appear, auto-rename them.\r\n    \"\"\"\r\n    struct_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StructType)]\r\n    if not struct_cols:\r\n        return df, False\r\n\r\n    exprs = []\r\n    used = set()\r\n\r\n    for field in df.schema.fields:\r\n        name = field.name\r\n        dtype = field.dataType\r\n\r\n        if isinstance(dtype, T.StructType):\r\n            # flatten each field inside struct\r\n            for inner in dtype.fields:\r\n                new_name = inner.name\r\n\r\n                # auto-fix duplicate names\r\n                if new_name in used:\r\n                    i = 1\r\n                    while f\"{new_name}_{i}\" in used:\r\n                        i += 1\r\n                    new_name = f\"{new_name}_{i}\"\r\n\r\n                exprs.append(F.col(f\"{name}.{inner.name}\").alias(new_name))\r\n                used.add(new_name)\r\n\r\n        else:\r\n            # keep regular columns\r\n            col_name = name\r\n            if col_name in used:\r\n                # auto-fix duplicates\r\n                i = 1\r\n                while f\"{col_name}_{i}\" in used:\r\n                    i += 1\r\n                col_name = f\"{col_name}_{i}\"\r\n\r\n            exprs.append(F.col(name).alias(col_name))\r\n            used.add(col_name)\r\n\r\n    return df.select(*exprs), True\r\n\r\ndef explode_arrays(df, explode_arrays_of_structs=True, join_primitive_arrays=False, join_sep=\"|\"):\r\n    \"\"\"\r\n    Explode top-level array columns of StructType into rows.\r\n    - explode_arrays_of_structs: if True, arrays of structs are exploded (one row per element)\r\n    - join_primitive_arrays: if True, primitive arrays (array<string/int>) are converted to joined strings; else kept as arrays\r\n    Returns (df, changed)\r\n    \"\"\"\r\n    arr_fields = [f for f in df.schema.fields if _is_array(f.dataType)]\r\n    if not arr_fields:\r\n        return df, False\r\n\r\n    # prefer to explode the first array field (to avoid exploding multiple arrays and generating huge cartesian)\r\n    for f in arr_fields:\r\n        name = f.name\r\n        elem_type = f.dataType.elementType\r\n        if isinstance(elem_type, T.StructType) and explode_arrays_of_structs:\r\n            df = df.withColumn(name, F.explode_outer(F.col(name)))\r\n            return df, True\r\n        else:\r\n            # primitive arrays\r\n            if join_primitive_arrays:\r\n                df = df.withColumn(name, F.when(F.col(name).isNotNull(), F.concat_ws(join_sep, F.col(name))).otherwise(None))\r\n                return df, True\r\n            # keep arrays as-is (no explode) — return unchanged for primitive arrays\r\n    return df, False\r\n\r\ndef normalize_nested_json(df,\r\n                          explode_arrays_of_structs=True,\r\n                          join_primitive_arrays=False,\r\n                          max_iters=20):\r\n    \"\"\"\r\n    Flatten structs fully (no prefix) and optionally explode arrays.\r\n    \"\"\"\r\n    it = 0\r\n    changed = True\r\n    while changed and it < max_iters:\r\n        it += 1\r\n        changed = False\r\n\r\n        # flatten structs (now without prefix)\r\n        df, c1 = flatten_structs(df)\r\n        changed = changed or c1\r\n\r\n        # explode array-of-structs\r\n        df, c2 = explode_arrays(df,\r\n                                explode_arrays_of_structs=explode_arrays_of_structs,\r\n                                join_primitive_arrays=join_primitive_arrays)\r\n        changed = changed or c2\r\n\r\n    return df\r\n#================Flatten JSON END ===================================\r\n\r\n# ===== DATA PROCESSING SECTION =====\r\ndef copy_parse_dedupe(spark, md, s3_staging_dir=None):\r\n    fs = {k.lower(): v for k, v in md.get(\"file_settings\", {}).items()}\r\n\r\n    source_path = md[\"source\"][\"SOURCEPATH\"]\r\n    logger.info(f\"copy_parse_dedupe - source path: {source_path}\")\r\n\r\n    # ---- OPTION 2 CHANGE: normalize 'sql' style aliases to Azure SQL ----\r\n    source_type_raw = md[\"source\"].get(\"SOURCE_TYPE\")\r\n    source_type = (source_type_raw or \"\").lower()\r\n    logger.info(f\"copy_parse_dedupe - source type: {source_type}\")\r\n\r\n    if source_type in [\"sql\", \"mssql\", \"azure_sql\"]:\r\n        logger.info(\r\n            f\"Normalizing source type '{source_type}' to 'azuresql' for JDBC handling.\"\r\n        )\r\n        source_type = \"azuresql\"\r\n    # --------------------------------------------------------------------\r\n\r\n    # <<< NEW: capture format type for awss3/xlsx support >>>\r\n    format_type = (md[\"source\"].get(\"FORMAT_TYPE\") or \"\").lower()\r\n    logger.info(f\"copy_parse_dedupe - format type: {format_type}\")\r\n\r\n    try:\r\n        read_options = {}\r\n        if fs.get(\"delimiter\"):\r\n            read_options[\"sep\"] = fs[\"delimiter\"]\r\n        if fs.get(\"skipheader\"):\r\n            read_options[\"header\"] = \"true\" if int(fs[\"skipheader\"]) > 0 else \"false\"\r\n        if fs.get(\"quotechar\"):\r\n            read_options[\"quote\"] = fs[\"quotechar\"]\r\n        if fs.get(\"multiline\"):\r\n            read_options[\"multiline\"] = \"true\" if int(fs[\"multiline\"]) > 0 else \"false\"\r\n        logger.info(\"Read options prepared successfully.\")\r\n    except Exception as e:\r\n        logger.error(f\"Failed to prepare read options: {str(e)}\")\r\n        raise\r\n\r\n    try:\r\n        # JDBC sources\r\n        if source_type in [\r\n            \"jdbc\",\r\n            \"sqlserver\",\r\n            \"postgresql\",\r\n            \"mysql\",\r\n            \"oracle\",\r\n            \"redshift\",\r\n            \"azuresql\",\r\n            \"saphana\",\r\n        ]:\r\n            secret = get_secret_cached(md[\"source\"][\"SECRET_KEY\"])\r\n            url, driver = build_jdbc_url(source_type, secret)\r\n\r\n            df = (\r\n                spark.read.format(\"jdbc\")\r\n                .option(\"url\", url)\r\n                .option(\"dbtable\", source_path)\r\n                .option(\"user\", secret[\"username\"])\r\n                .option(\"password\", secret[\"password\"])\r\n                .option(\"driver\", driver)\r\n                .load()\r\n            )\r\n\r\n        # Salesforce\r\n        elif source_type in [\"salesforce\", \"sf\", \"sfdc\"]:\r\n            sf_secret = get_secret_cached(md[\"source\"][\"SECRET_KEY\"])\r\n            sf_username = sf_secret.get(\"username\")\r\n            sf_password = sf_secret.get(\"password\")\r\n            sf_token = (\r\n                sf_secret.get(\"security_token\") or sf_secret.get(\"token\") or \"\"\r\n            )\r\n            sf_login = sf_secret.get(\"login_url\") or \"https://login.salesforce.com\"\r\n            sf_version = sf_secret.get(\"version\") or \"60.0\"\r\n\r\n            if not sf_username or not sf_password:\r\n                raise ValueError(\r\n                    \"Salesforce secret must include 'username' and 'password' (and usually 'security_token').\"\r\n                )\r\n\r\n            sf_pwd_concat = f\"{sf_password}{sf_token}\"\r\n\r\n            reader = (\r\n                spark.read.format(\"com.springml.spark.salesforce\")\r\n                .option(\"username\", sf_username)\r\n                .option(\"password\", sf_pwd_concat)\r\n                .option(\"login\", sf_login)\r\n                .option(\"version\", sf_version)\r\n            )\r\n\r\n            sp_raw = (source_path or \"\").strip()\r\n            if sp_raw.lower().startswith(\"select\"):\r\n                df = reader.option(\"soql\", sp_raw).load()\r\n            else:\r\n                mapped_src_cols = [\r\n                    (m.get(\"SOURCECOLUMNNAME\") or \"\").strip()\r\n                    for m in md.get(\"mappings\", [])\r\n                ]\r\n                mapped_src_cols = [c for c in mapped_src_cols if c]\r\n                select_cols = (\r\n                    \", \".join(sorted(set(mapped_src_cols)))\r\n                    if mapped_src_cols\r\n                    else \"Id\"\r\n                )\r\n                soql = f\"SELECT {select_cols} FROM {sp_raw}\"\r\n                logger.info(f\"Built SOQL for Salesforce read: {soql}\")\r\n                df = reader.option(\"soql\", soql).load()\r\n\r\n            logger.info(\"Successfully read data from Salesforce\")\r\n\r\n        # ==== HUBSPOT READER (contacts / companies via REST API) ====\r\n        elif source_type in [\"hubspot\", \"hubspotapi\", \"hubspot_rest\"]:\r\n            hs_secret = get_secret_cached(md[\"source\"][\"SECRET_KEY\"])\r\n\r\n            token = (\r\n                hs_secret.get(\"access_token\")\r\n                or hs_secret.get(\"AccessToken\")\r\n                or hs_secret.get(\"token\")\r\n                or hs_secret.get(\"pat\")\r\n            )\r\n            if not token:\r\n                raise ValueError(\r\n                    \"HubSpot secret must contain one of: AccessToken, access_token, token, pat\"\r\n                )\r\n\r\n            base_url = (\r\n                hs_secret.get(\"base_url\")\r\n                or hs_secret.get(\"BASE_URL\")\r\n                or \"https://api.hubapi.com\"\r\n            )\r\n\r\n            object_name = (md[\"source\"][\"SOURCEPATH\"] or \"contacts\").strip()\r\n\r\n            mapped_src_cols = [\r\n                (m.get(\"SOURCECOLUMNNAME\") or \"\").strip()\r\n                for m in md.get(\"mappings\", [])\r\n            ]\r\n            mapped_src_cols = [c for c in mapped_src_cols if c]\r\n\r\n            api_property_keys = [\r\n                c.lower()\r\n                for c in mapped_src_cols\r\n                if c.lower() not in [\"recordid\", \"id\", \"hs_object_id\"]\r\n            ]\r\n\r\n            url = f\"{base_url.rstrip('/')}/crm/v3/objects/{object_name}\"\r\n            headers = {\r\n                \"Authorization\": f\"Bearer {token}\",\r\n                \"Content-Type\": \"application/json\",\r\n            }\r\n\r\n            all_rows = []\r\n            after = None\r\n\r\n            while True:\r\n                params = {\"limit\": 100}\r\n                if api_property_keys:\r\n                    params[\"properties\"] = \",\".join(api_property_keys)\r\n                if after:\r\n                    params[\"after\"] = after\r\n\r\n                resp = requests.get(url, headers=headers, params=params)\r\n                if resp.status_code != 200:\r\n                    raise RuntimeError(\r\n                        f\"HubSpot API error {resp.status_code}: {resp.text}\"\r\n                    )\r\n\r\n                data = resp.json()\r\n\r\n                for item in data.get(\"results\", []):\r\n                    props = item.get(\"properties\", {}) or {}\r\n                    row = {}\r\n                    for src_name in mapped_src_cols:\r\n                        key_l = src_name.lower()\r\n                        if key_l in [\"recordid\", \"id\", \"hs_object_id\"]:\r\n                            row[src_name] = item.get(\"id\")\r\n                        else:\r\n                            row[src_name] = props.get(key_l)\r\n                    all_rows.append(row)\r\n\r\n                paging = data.get(\"paging\") or {}\r\n                after = (paging.get(\"next\") or {}).get(\"after\")\r\n                if not after:\r\n                    break\r\n\r\n            schema = StructType(\r\n                [StructField(c, StringType(), True) for c in mapped_src_cols]\r\n            )\r\n\r\n            df = spark.createDataFrame(all_rows or [], schema=schema)\r\n\r\n            logger.info(\r\n                f\"Successfully read {df.count()} rows from HubSpot {object_name}\"\r\n            )\r\n\r\n        # ==== S3 via SourceType = awss3 (XLSX / Excel / CSV / JSON) ====\r\n        elif source_type in [\"awss3\",\"aws_s3\"]:\r\n            s3_secret = get_secret_cached(md[\"source\"][\"SECRET_KEY\"])\r\n\r\n            path = source_path\r\n            if path.startswith(\"s3://\"):\r\n                tmp = path[5:]\r\n                bucket, key = tmp.split(\"/\", 1)\r\n            else:\r\n                bucket = s3_secret[\"bucket\"]\r\n                key = path.lstrip(\"/\")\r\n\r\n            s3_region = s3_secret.get(\"region_name\", DEFAULT_AWS_REGION)\r\n            s3_client = boto3.client(\"s3\", region_name=s3_region)\r\n\r\n            logger.info(\r\n                f\"Reading S3 object bucket={bucket}, key={key}, region={s3_region}\"\r\n            )\r\n            obj = s3_client.get_object(Bucket=bucket, Key=key)\r\n            body = obj[\"Body\"].read()\r\n\r\n            lower_key = key.lower()\r\n\r\n            if format_type in [\"xlsx\", \"excel\", \"xls\"] or lower_key.endswith(\r\n                (\".xlsx\", \".xls\")\r\n            ):\r\n                df_pd = pd.read_excel(BytesIO(body))\r\n                df = spark.createDataFrame(df_pd)\r\n\r\n            else:\r\n                raise ValueError(\r\n                    f\"Unsupported S3 format for awss3 source: format_type='{format_type}', key='{key}'\"\r\n                )\r\n\r\n            logger.info(\"S3 (awss3) read success.\")\r\n\r\n        # ==== Azure Blob via SourceType = azureblob (XLSX) ====\r\n        elif source_type in [\"azureblob\",\"azure_blob\"]:\r\n            df = read_from_azureblob_xlsx(spark, md)\r\n\r\n        # SFTP\r\n        elif source_type == \"sftp\":\r\n            if read_options.get(\"header\") == \"true\":\r\n                read_options[\"header\"] = 0\r\n\r\n            if \"quote\" in read_options:\r\n                read_options[\"quotechar\"] = read_options.pop(\"quote\")\r\n\r\n            secret_name = md[\"source\"][\"SECRET_KEY\"]\r\n            sftp_secret = get_secret_cached(secret_name)\r\n\r\n            client = paramiko.SSHClient()\r\n            client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\r\n            client.connect(\r\n                sftp_secret[\"host\"],\r\n                sftp_secret[\"port\"],\r\n                sftp_secret[\"username\"],\r\n                sftp_secret[\"password\"],\r\n            )\r\n\r\n            sftp = client.open_sftp()\r\n            with sftp.open(source_path, \"r\") as f:\r\n                file_data = f.read()\r\n\r\n            if source_path.endswith(\".csv\"):\r\n                text_data = file_data.decode(\"utf-8\")\r\n                df_pd = pd.read_csv(io.StringIO(text_data), **read_options)\r\n                df = spark.createDataFrame(df_pd)\r\n\r\n            elif source_path.endswith(\".json\"):\r\n                # read raw JSON text\r\n                text_data = file_data.decode(\"utf-8\")\r\n            \r\n                # create an RDD and read as JSON (honors multiline option)\r\n                rdd = spark.sparkContext.parallelize([text_data])\r\n                multiline_flag = str(read_options.get(\"multiline\", \"false\")).lower()\r\n                df = spark.read.option(\"multiline\", multiline_flag).json(rdd)\r\n\r\n                # run normalization (flatten structs and explode arrays progressively)\r\n                df = normalize_nested_json(df)\r\n            \r\n                # show result (use show(truncate=False) to inspect full values)\r\n                logger.info(f\"After normalizing JSON, columns: {df.columns}\")\r\n                df.show(truncate=False)\r\n                \r\n            elif source_path.endswith(\".psv\"):\r\n                text_data = file_data.decode(\"utf-8\")\r\n                df_pd = pd.read_csv(io.StringIO(text_data), **read_options)\r\n                df = spark.createDataFrame(df_pd)\r\n            \r\n            elif source_path.endswith(\".parquet\"):\r\n                try:\r\n                    # read bytes from sftp in binary mode\r\n                    with sftp.open(source_path, \"rb\") as fb:\r\n                        file_bytes = fb.read()\r\n                except Exception:\r\n                    file_bytes = file_data if isinstance(file_data, (bytes, bytearray)) else str(file_data).encode(\"utf-8\")\r\n\r\n                b = io.BytesIO(file_bytes)\r\n                table = pq.read_table(b)\r\n                df_pd = table.to_pandas()\r\n                df = spark.createDataFrame(df_pd)\r\n                \r\n        # ---------- START: Dynamic per-pipeline REST API integration (drop-in replace) ----------\r\n        elif source_type in [\"restapi\", \"api\", \"http\", \"rest\"]:\r\n            print(\"api - Process start\")\r\n            # ensure token_cache exists in module scope\r\n            try:\r\n                token_cache\r\n            except NameError:\r\n                token_cache = {}\r\n                \r\n            # ---------- START: robust normalizer for headers / params ----------\r\n            def _normalize_to_dict(obj, name=\"headers\"):\r\n                \"\"\"\r\n                Accepts:\r\n                  - dict -> returned as-is\r\n                  - JSON string -> json.loads -> dict (if possible)\r\n                  - list of \"Key: Value\" strings -> parsed into dict\r\n                  - list/iterable of (k,v) pairs -> dict()\r\n                  - None -> {}\r\n                Returns a dict.\r\n                \"\"\"\r\n                if obj is None:\r\n                    return {}\r\n                # already a dict\r\n                if isinstance(obj, dict):\r\n                    return obj\r\n                # JSON string\r\n                if isinstance(obj, str):\r\n                    s = obj.strip()\r\n                    # try load JSON\r\n                    try:\r\n                        parsed = json.loads(s)\r\n                        if isinstance(parsed, dict):\r\n                            return parsed\r\n                    except Exception:\r\n                        pass\r\n                    # try parse single header style \"k:v, k2:v2\" or \"k: v\"\r\n                    # split on commas only if it looks like multiple headers\r\n                    try:\r\n                        items = [s] if (\":\" in s and \",\" not in s) else s.split(\",\")\r\n                        out = {}\r\n                        for it in items:\r\n                            if not it:\r\n                                continue\r\n                            if \":\" in it:\r\n                                k, v = it.split(\":\", 1)\r\n                                out[k.strip()] = v.strip()\r\n                            else:\r\n                                # fallback: single token -> skip\r\n                                continue\r\n                        if out:\r\n                            return out\r\n                    except Exception:\r\n                        pass\r\n                    return {}\r\n                # list/iterable case\r\n                if isinstance(obj, (list, tuple, set)):\r\n                    out = {}\r\n                    # list of \"Key: Value\" strings\r\n                    if all(isinstance(x, str) for x in obj):\r\n                        for it in obj:\r\n                            if not it:\r\n                                continue\r\n                            if \":\" in it:\r\n                                k, v = it.split(\":\", 1)\r\n                                out[k.strip()] = v.strip()\r\n                            else:\r\n                                # can't parse single token, skip\r\n                                continue\r\n                        return out\r\n                    # list of pairs\r\n                    try:\r\n                        out = dict(obj)\r\n                        if isinstance(out, dict):\r\n                            return out\r\n                    except Exception:\r\n                        # fallthrough\r\n                        pass\r\n                # unknown type -> return empty dict\r\n                logger.warning(f\"[RESTAPI] {_normalize_to_dict.__name__}: could not normalize {name} (type={type(obj)}); using empty dict\")\r\n                return {}\r\n            # ---------- END: robust normalizer for headers / params ----------\r\n        \r\n            # Helper: normalize/merge config sources (md.source, md.properties, secret)\r\n            def _gather_api_config(md):\r\n                \"\"\"\r\n                Returns a dict `cfg` with keys used below:\r\n                - endpoint/base_url/token_endpoint/token_method/token_payload/token_headers\r\n                - access_token, auth_scheme, api_key_header, default_headers, default_params\r\n                - pagination_type, pagination_param, results_path, page_size, timeout\r\n                Sources of truth (priority):\r\n                 1) pipeline SOURCE table fields and md['properties'] (if present)\r\n                 2) secret content (if SECRET_KEY provided)\r\n                 3) reasonable defaults\r\n                \"\"\"\r\n                cfg = {}\r\n                # 1) pipeline-level config - expect it in md['source'] and md.get('properties')\r\n                pipeline_src = md.get(\"source\") or {}\r\n                properties = md.get(\"file_settings\") or {}\r\n        \r\n                # copy known keys from pipeline config (if present)\r\n                for k in [\"base_url\", \"token_endpoint\", \"token_method\", \"token_payload\",\r\n                          \"token_headers\", \"access_token\", \"auth_scheme\", \"api_key_header\",\r\n                          \"default_headers\", \"default_params\", \"pagination_type\", \"pagination_param\",\r\n                          \"results_path\", \"page_size\", \"timeout\", \"start_page\", \"page_size\"]:\r\n                    if pipeline_src.get(k) is not None:\r\n                        cfg[k] = pipeline_src.get(k)\r\n                    elif properties.get(k) is not None:\r\n                        cfg[k] = properties.get(k)\r\n        \r\n                # 2) secret fallback\r\n                secret_name = pipeline_src.get(\"SECRET_KEY\") or pipeline_src.get(\"SECRETID\") or pipeline_src.get(\"SECRET\")\r\n                secret_content = {}\r\n                if secret_name:\r\n                    try:\r\n                        secret_content = get_secret_cached(secret_name) or {}\r\n                    except Exception:\r\n                        secret_content = {}\r\n                # merge secret values where pipeline didn't supply\r\n                for k in [\"base_url\", \"token_endpoint\", \"token_method\", \"token_payload\",\r\n                          \"token_headers\", \"access_token\", \"auth_scheme\", \"api_key_header\",\r\n                          \"default_headers\", \"default_params\", \"pagination_type\", \"pagination_param\",\r\n                          \"results_path\", \"page_size\", \"timeout\", \"start_page\"]:\r\n                    if cfg.get(k) is None and secret_content.get(k) is not None:\r\n                        cfg[k] = secret_content.get(k)\r\n        \r\n                # final normalization & defaults\r\n                cfg[\"base_url\"] = (cfg.get(\"base_url\") or \"\").rstrip(\"/\") if cfg.get(\"base_url\") else \"\"\r\n                cfg[\"token_method\"] = (cfg.get(\"token_method\") or \"POST\").upper()\r\n                cfg[\"auth_scheme\"] = (cfg.get(\"auth_scheme\") or \"Bearer\")\r\n                cfg[\"api_key_header\"] = cfg.get(\"api_key_header\") or \"x-api-key\"\r\n                cfg[\"default_headers\"] = cfg.get(\"default_headers\") or {}\r\n                cfg[\"default_params\"] = cfg.get(\"default_params\") or {}\r\n                cfg[\"pagination_type\"] = (cfg.get(\"pagination_type\") or \"\").lower()\r\n                cfg[\"pagination_param\"] = cfg.get(\"pagination_param\") or cfg.get(\"page_param\") or \"page\"\r\n                cfg[\"results_path\"] = cfg.get(\"results_path\") or None\r\n                cfg[\"page_size\"] = int(cfg.get(\"page_size\") or 100)\r\n                cfg[\"timeout\"] = int(cfg.get(\"timeout\") or DEFAULTS.get(\"DEFAULT_TIMEOUT\", 30))\r\n                cfg[\"_secret_name\"] = secret_name\r\n                cfg[\"_secret_content\"] = secret_content\r\n                return cfg\r\n        \r\n            # Helper: call API with 401-refresh attempt (uses token_endpoint from cfg)\r\n            def _api_call_with_refresh(endpoint, method=\"GET\", headers=None, params=None, payload=None, cfg=None, cache_key=None, max_retries=1):\r\n                \"\"\"\r\n                Call api_request; if a 401/403 occurs and token_endpoint is configured,\r\n                attempt to regenerate token once and retry.\r\n            \r\n                This version ensures authType and Token are passed to api_request when available,\r\n                and also places the token in headers as a fallback.\r\n                \"\"\"\r\n                headers = dict(headers or {})\r\n                params = dict(params or {})\r\n                attempt = 0\r\n            \r\n                # Helper to resolve token from cfg, secret, or cache\r\n                def _resolve_token():\r\n                    # 1) explicit access_token in cfg or secret content\r\n                    tok = cfg.get(\"access_token\") or (cfg.get(\"_secret_content\") or {}).get(\"access_token\")\r\n                    # 2) cached token per pipeline/secret\r\n                    if not tok and cache_key and cache_key in token_cache:\r\n                        tok = token_cache[cache_key]\r\n                    return tok\r\n            \r\n                # Helper to inject token into headers based on scheme\r\n                def _inject_token_into_headers(token_value):\r\n                    if not token_value:\r\n                        return\r\n                    auth_scheme = (cfg.get(\"auth_scheme\") or \"Bearer\").upper()\r\n                    if auth_scheme == \"JWT\":\r\n                        headers.setdefault(\"Authorization\", f\"JWT {token_value}\")\r\n                    elif auth_scheme in (\"BEARER\", \"OAUTH\", \"OAUTHTOKEN\"):\r\n                        headers.setdefault(\"Authorization\", f\"Bearer {token_value}\")\r\n                    elif auth_scheme == \"API_KEY\":\r\n                        # allow custom api_key_header in cfg or secret; fallback to x-api-key\r\n                        api_key_header = cfg.get(\"api_key_header\") or (cfg.get(\"_secret_content\") or {}).get(\"api_key_header\") or \"x-api-key\"\r\n                        headers.setdefault(api_key_header, token_value)\r\n                    else:\r\n                        # support passing raw header name (e.g., \"X-Auth-Token\") if auth_scheme is not standard\r\n                        if isinstance(cfg.get(\"auth_scheme\"), str) and \":\" in cfg.get(\"auth_scheme\"):\r\n                            # format \"HeaderName:SCHEME\" not common but keep fallback\r\n                            hdr = cfg.get(\"auth_scheme\").split(\":\", 1)[0].strip()\r\n                            headers.setdefault(hdr, token_value)\r\n                        else:\r\n                            headers.setdefault(\"Authorization\", f\"Bearer {token_value}\")\r\n            \r\n                # initial resolution/injection\r\n                token_value = _resolve_token()\r\n                _inject_token_into_headers(token_value)\r\n            \r\n                # Also set authType/Token params expected by api_request if possible\r\n                # Map our auth_scheme to api_request's authType\r\n                def _map_auth_type():\r\n                    ascheme = (cfg.get(\"auth_scheme\") or \"Bearer\")\r\n                    if isinstance(ascheme, str):\r\n                        a = ascheme.upper()\r\n                        if a in (\"BEARER\", \"JWT\", \"API_KEY\"):\r\n                            return a\r\n                    # default fallback\r\n                    return \"Bearer\"\r\n            \r\n                authType = _map_auth_type()\r\n                Token = token_value\r\n            \r\n                while attempt <= max_retries:\r\n                    attempt += 1\r\n                    # call api_request with both header and explicit auth args (ensures compatibility)\r\n                    resp = api_request(\r\n                        endpoint,\r\n                        method=method,\r\n                        authType=authType,\r\n                        Token=Token,\r\n                        headers=headers,\r\n                        params=params,\r\n                        payload=payload,\r\n                        timeout=cfg.get(\"timeout\"),\r\n                        raise_for_status=False,\r\n                    )\r\n            \r\n                    status_code = resp.get(\"status_code\") if isinstance(resp, dict) else None\r\n            \r\n                    # success -> return\r\n                    if status_code is None or (200 <= status_code < 300):\r\n                        return resp\r\n            \r\n                    # on 401/403 attempt refresh and retry (only if token_endpoint is configured)\r\n                    if status_code in (401, 403) and attempt <= max_retries:\r\n                        logger.info(f\"[RESTAPI] {status_code} received for {endpoint}. Attempting token refresh (attempt {attempt}).\")\r\n                        new_token = None\r\n                        token_endpoint = cfg.get(\"token_endpoint\") or (cfg.get(\"_secret_content\") or {}).get(\"token_endpoint\")\r\n                        if token_endpoint:\r\n                            # fallback: direct call to token endpoint then attempt to extract token\r\n                            if not new_token:\r\n                                try:\r\n                                    token_method = (cfg.get(\"token_method\") or \"POST\").upper()\r\n                                    token_payload = cfg.get(\"token_payload\") or (cfg.get(\"_secret_content\") or {}).get(\"token_payload\")\r\n                                    token_headers = cfg.get(\"token_headers\") or (cfg.get(\"_secret_content\") or {}).get(\"token_headers\")\r\n                                    token_resp = api_request(\r\n                                        token_endpoint,\r\n                                        method=token_method,\r\n                                        authType=None,\r\n                                        Token=None,\r\n                                        headers=token_headers,\r\n                                        params=None,\r\n                                        payload=(token_payload if isinstance(token_payload, dict) else (json.loads(token_payload) if isinstance(token_payload, str) else token_payload)),\r\n                                        timeout=int(cfg.get(\"token_timeout\") or cfg.get(\"timeout\") or 30),\r\n                                        raise_for_status=False,\r\n                                    )\r\n                                    new_token = extract_token_from_response(token_resp)\r\n                                    # if extract_token_from_response couldn't find token but token_resp contains access_token key, then use it\r\n                                    if not new_token and isinstance(token_resp, dict):\r\n                                        b = token_resp.get(\"body\")\r\n                                        if isinstance(b, dict):\r\n                                            new_token = b.get(\"access_token\") or b.get(\"token\") or b.get(\"token_value\")\r\n                                except Exception as e:\r\n                                    logger.warning(f\"[RESTAPI] token endpoint call failed during refresh: {e}\")\r\n            \r\n                        if new_token:\r\n                            # update in-memory cache and headers for subsequent attempts\r\n                            if cache_key:\r\n                                token_cache[cache_key] = new_token\r\n                            Token = new_token\r\n                            _inject_token_into_headers(new_token)\r\n                            authType = _map_auth_type()\r\n                            logger.info(\"[RESTAPI] Token refresh succeeded; retrying API call.\")\r\n                            # retry loop continues\r\n                            continue\r\n                        else:\r\n                            logger.warning(\"[RESTAPI] Token refresh failed or not configured; returning original response.\")\r\n                            return resp\r\n                    else:\r\n                        # non-auth error or retries exhausted\r\n                        return resp\r\n                return resp\r\n        \r\n            # Gather pipeline-specific config (md -> secret)\r\n            cfg = _gather_api_config(md)\r\n            secret_name = cfg.get(\"_secret_name\")\r\n        \r\n            # Build endpoint (SOURCEPATH may be full path or relative)\r\n            full_path = (source_path or \"\").strip()\r\n            if full_path.lower().startswith(\"http\"):\r\n                endpoint = full_path\r\n            else:\r\n                endpoint = f\"{cfg['base_url']}/{full_path.lstrip('/')}\" if cfg.get(\"base_url\") else full_path\r\n            \r\n            # Prepare headers and params (merge default headers/params from cfg)\r\n            _headers_from_cfg = _normalize_to_dict(cfg.get(\"default_headers\"), \"default_headers\")\r\n            _headers_from_secret = _normalize_to_dict(cfg.get(\"_secret_content\", {}).get(\"default_headers\"), \"secret.default_headers\")\r\n            # pipeline config takes precedence, but merge secret fallback for anything missing\r\n            headers = {}\r\n            headers.update(_headers_from_secret)\r\n            headers.update(_headers_from_cfg)\r\n            \r\n            _params_from_cfg = _normalize_to_dict(cfg.get(\"default_params\"), \"default_params\")\r\n            _params_from_secret = _normalize_to_dict(cfg.get(\"_secret_content\", {}).get(\"default_params\"), \"secret.default_params\")\r\n            params = {}\r\n            params.update(_params_from_secret)\r\n            params.update(_params_from_cfg)\r\n        \r\n            # Determine token: (1) pipeline-provided static token, (2) secret's access_token, (3) cached token, (4) token_endpoint to generate\r\n            token = cfg.get(\"access_token\") or (cfg.get(\"_secret_content\") or {}).get(\"access_token\")\r\n            cache_key = None\r\n            # prefer per-pipeline cache key if SOURCE table has unique id (SOURCEID or SOURCENAME)\r\n            pipeline_key = None\r\n            if md.get(\"source\", {}).get(\"SOURCEID\"):\r\n                pipeline_key = str(md.get(\"source\", {}).get(\"SOURCEID\"))\r\n            elif md.get(\"source\", {}).get(\"SOURCENAME\"):\r\n                pipeline_key = str(md.get(\"source\", {}).get(\"SOURCENAME\"))\r\n            cache_key = f\"token:{pipeline_key}\" if pipeline_key else (f\"token:{secret_name}\" if secret_name else None)\r\n        \r\n            if not token and cache_key and cache_key in token_cache:\r\n                token = token_cache[cache_key]\r\n        \r\n            # If still no token but token_endpoint exists in cfg, generate and cache it\r\n            if not token and cfg.get(\"token_endpoint\"):\r\n                try:\r\n                    new_token = None\r\n                    if not new_token:\r\n                        token_resp = api_request(\r\n                            cfg.get(\"token_endpoint\"),\r\n                            method=cfg.get(\"token_method\", \"POST\"),\r\n                            authType=None,\r\n                            Token=None,\r\n                            headers=cfg.get(\"token_headers\") or cfg.get(\"_secret_content\", {}).get(\"token_headers\"),\r\n                            payload=(cfg.get(\"token_payload\") or cfg.get(\"_secret_content\", {}).get(\"token_payload\")),\r\n                            timeout=cfg.get(\"timeout\"),\r\n                            raise_for_status=False,\r\n                        )\r\n                        new_token = extract_token_from_response(token_resp)\r\n                    if new_token:\r\n                        token = new_token\r\n                        if cache_key:\r\n                            token_cache[cache_key] = new_token\r\n                        logger.info(f\"[RESTAPI] Generated token dynamically for pipeline (cache_key={cache_key})\")\r\n                except Exception as e:\r\n                    logger.warning(f\"[RESTAPI] Failed generating token dynamically: {e}\")\r\n        \r\n            # Place token into headers according to auth scheme\r\n            if token:\r\n                auth_scheme = (cfg.get(\"auth_scheme\") or \"Bearer\").upper()\r\n                if auth_scheme == \"JWT\":\r\n                    headers.setdefault(\"Authorization\", f\"JWT {token}\")\r\n                elif auth_scheme in (\"BEARER\", \"OAUTH\", \"OAUTHTOKEN\"):\r\n                    headers.setdefault(\"Authorization\", f\"Bearer {token}\")\r\n                elif auth_scheme == \"API_KEY\":\r\n                    headers.setdefault(cfg.get(\"api_key_header\") or \"x-api-key\", token)\r\n                else:\r\n                    headers.setdefault(\"Authorization\", f\"Bearer {token}\")\r\n        \r\n            # mapped_src_cols from md.mappings (same as your existing flow)\r\n            mapped_src_cols = [(m.get(\"SOURCECOLUMNNAME\") or \"\").strip() for m in md.get(\"mappings\", [])]\r\n            mapped_src_cols = [c for c in mapped_src_cols if c]\r\n        \r\n            # Pagination + read loop (uses _api_call_with_refresh to handle 401 -> refresh)\r\n            all_rows = []\r\n            next_cursor = None\r\n            page = int(cfg.get(\"start_page\") or 1)\r\n        \r\n            loop_params = dict(params)  # use default params only\r\n\r\n            resp = _api_call_with_refresh(\r\n                endpoint,\r\n                method=\"GET\",\r\n                headers=headers,\r\n                params=loop_params,\r\n                payload=None,\r\n                cfg=cfg,\r\n                cache_key=cache_key,\r\n                max_retries=1\r\n            )\r\n\r\n            # Error handling\r\n            if isinstance(resp, dict) and resp.get(\"error\"):\r\n                raise RuntimeError(f\"[RESTAPI] API error while calling {endpoint}: {resp.get('error')}\")\r\n\r\n            status_code = resp.get(\"status_code\")\r\n            body = resp.get(\"body\")\r\n            if status_code and (status_code < 200 or status_code >= 300):\r\n                raise RuntimeError(f\"[RESTAPI] Unexpected status {status_code} for {endpoint}: {body}\")\r\n\r\n            # ---------- Convert API body -> Spark DataFrame (normalize nested JSON, no-prefix) ----------\r\n            def _is_struct(t): return isinstance(t, T.StructType)\r\n            def _is_array(t): return isinstance(t, T.ArrayType)\r\n\r\n            def flatten_structs_no_prefix(df):\r\n                struct_cols = [f.name for f in df.schema.fields if _is_struct(f.dataType)]\r\n                if not struct_cols:\r\n                    return df, False\r\n                used = set()\r\n                exprs = []\r\n                for f in df.schema.fields:\r\n                    name, dtype = f.name, f.dataType\r\n                    if _is_struct(dtype):\r\n                        for inner in dtype.fields:\r\n                            new_name = inner.name\r\n                            if new_name in used:\r\n                                i = 1\r\n                                while f\"{new_name}_{i}\" in used:\r\n                                    i += 1\r\n                                new_name = f\"{new_name}_{i}\"\r\n                            exprs.append(F.col(f\"{name}.{inner.name}\").alias(new_name))\r\n                            used.add(new_name)\r\n                    else:\r\n                        col_name = name\r\n                        if col_name in used:\r\n                            i = 1\r\n                            while f\"{col_name}_{i}\" in used:\r\n                                i += 1\r\n                            col_name = f\"{col_name}_{i}\"\r\n                        exprs.append(F.col(name).alias(col_name))\r\n                        used.add(col_name)\r\n                return df.select(*exprs), True\r\n\r\n            def explode_arrays(df):\r\n                arr_fields = [f for f in df.schema.fields if _is_array(f.dataType)]\r\n                if not arr_fields:\r\n                    return df, False\r\n                for f in arr_fields:\r\n                    elem = f.dataType.elementType\r\n                    if isinstance(elem, T.StructType):\r\n                        df = df.withColumn(f.name, F.explode_outer(F.col(f.name)))\r\n                        return df, True\r\n                return df, False\r\n\r\n            def normalize_nested_json(df, max_iters=20):\r\n                changed = True\r\n                it = 0\r\n                while changed and it < max_iters:\r\n                    it += 1\r\n                    changed = False\r\n                    df, c1 = flatten_structs_no_prefix(df)\r\n                    df, c2 = explode_arrays(df)\r\n                    changed = c1 or c2\r\n                return df\r\n\r\n            def body_to_df(body):\r\n                # If JSON string, parse it\r\n                if isinstance(body, str):\r\n                    try:\r\n                        body_obj = json.loads(body)\r\n                    except Exception:\r\n                        return spark.createDataFrame([], T.StructType([]))\r\n                else:\r\n                    body_obj = body\r\n\r\n                # If dict -> single JSON object -> read as one-row DF\r\n                if isinstance(body_obj, dict):\r\n                    rdd = spark.sparkContext.parallelize([json.dumps(body_obj)])\r\n                    df = spark.read.json(rdd)\r\n                    return normalize_nested_json(df)\r\n\r\n                # If list -> array of JSON rows\r\n                if isinstance(body_obj, list):\r\n                    rdd = spark.sparkContext.parallelize([json.dumps(body_obj)])\r\n                    df = spark.read.json(rdd)\r\n                    # if Spark reads as single array column -> explode it\r\n                    if len(df.columns) == 1:\r\n                        col0 = df.columns[0]\r\n                        if isinstance(df.schema.fields[0].dataType, T.ArrayType):\r\n                            df = df.select(F.explode_outer(col0).alias(\"elem\")).select(\"elem.*\")\r\n                    return normalize_nested_json(df)\r\n\r\n                # fallback empty DF\r\n                return spark.createDataFrame([], T.StructType([]))\r\n\r\n            # produce normalized DF\r\n            df_from_api = body_to_df(body)\r\n\r\n            logger.info(f\"[RESTAPI] DataFrame columns: {df_from_api.columns}\")\r\n            df_from_api.show(truncate=False)\r\n\r\n            # assign this DF back to your variable used downstream\r\n            df = df_from_api\r\n            df.show()\r\n            logger.info(f\"[RESTAPI] Successfully read {df.count()} rows from {endpoint} for pipeline {md.get('source', {}).get('SOURCENAME')}\")\r\n        # ---------- END: Dynamic per-pipeline REST API integration ----------\r\n        else:\r\n            raise ValueError(\r\n                f\"Unsupported source type: {source_type}. Only JDBC, Salesforce, HubSpot, awss3, azureblob and SFTP connections are supported.\"\r\n            )\r\n\r\n        logger.info(f\"Successfully read data from source: {source_type}\")\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Error reading from source {source_type}: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n    try:\r\n        actual_by_lower = {c.lower(): c for c in df.columns}\r\n        mappings_sorted = sorted(\r\n            md[\"mappings\"],\r\n            key=lambda x: int((x.get(\"DESTINATIONCOLUMNSEQUENCE\") or 0)),\r\n        )\r\n\r\n        def _cast_for_dtype(col_expr, dtype):\r\n            dtype = (dtype or \"\").upper()\r\n            if \"DATE\" in dtype or \"TIMESTAMP\" in dtype:\r\n                return F.to_timestamp(col_expr)\r\n            elif \"INT\" in dtype:\r\n                return col_expr.cast(IntegerType())\r\n            elif any(x in dtype for x in [\"FLOAT\", \"DOUBLE\", \"DECIMAL\"]):\r\n                return col_expr.cast(DoubleType())\r\n            elif \"BOOLEAN\" in dtype:\r\n                return col_expr.cast(BooleanType())\r\n            else:\r\n                return col_expr.cast(StringType())\r\n\r\n        select_exprs = []\r\n        for m in mappings_sorted:\r\n            src_name = (m[\"SOURCECOLUMNNAME\"] or \"\").strip()\r\n            dest_name = (m[\"DESTINATIONCOLUMNNAME\"] or \"\").strip()\r\n            dtype = (m[\"DESTINATIONCOLUMNDATATYPE\"] or \"\").strip()\r\n\r\n            src_actual = actual_by_lower.get(src_name.lower())\r\n            if src_actual is None:\r\n                logger.warning(\r\n                    f\"Source column '{src_name}' not found in data; filling NULL as '{dest_name}'\"\r\n                )\r\n                col_expr = F.lit(None)\r\n            else:\r\n                col_expr = F.col(src_actual)\r\n\r\n            select_exprs.append(_cast_for_dtype(col_expr, dtype).alias(dest_name))\r\n\r\n        df = df.select(*select_exprs)\r\n        logger.info(\"Column selection, renaming, and casting completed successfully.\")\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Failed during column selection/renaming/casting: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n    try:\r\n        unique_keys = [\r\n            k.strip() for k in md[\"pipeline\"][\"UNIQUEKEYCOLUMNS\"].split(\",\")\r\n        ]\r\n        if unique_keys:\r\n            window_spec = Window.partitionBy(unique_keys).orderBy(\r\n                F.col(unique_keys[0]).desc()\r\n            )\r\n            df_dedup = (\r\n                df.withColumn(\"row_num\", F.row_number().over(window_spec))\r\n                .filter(F.col(\"row_num\") == 1)\r\n                .drop(\"row_num\")\r\n            )\r\n        else:\r\n            df_dedup = df.distinct()\r\n        logger.info(\"Deduplication completed successfully.\")\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Error during deduplication: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n    try:\r\n        if s3_staging_dir:\r\n            staging_path = f\"{s3_staging_dir}/staging/{md['destination']['DESTINATIONPATH'].replace('.', '/')}\"\r\n            df_dedup.write.mode(\"overwrite\").parquet(staging_path)\r\n            logger.info(f\"Staging data written to: {staging_path}\")\r\n    except Exception as e:\r\n        logger.warning(\r\n            f\"Staging write failed: {str(e)} — continuing without blocking\"\r\n        )\r\n\r\n    return df, df_dedup, unique_keys\r\n\r\n\r\n# ===== JDBC URL BUILDER SECTION =====\r\ndef build_jdbc_url(source_type, secret):\r\n    host = secret[\"host\"]\r\n    port = secret[\"port\"]\r\n    logger.info(f\"build_jdbc_url - host : {host}\")\r\n    logger.info(f\"build_jdbc_url - port : {port}\")\r\n    logger.info(f\"build_jdbc_url - building JDBC URL for source type: {source_type}\")\r\n\r\n    st = str(source_type).lower()\r\n\r\n    if st == \"sqlserver\":\r\n        database = secret.get(\"database\")\r\n        if database:\r\n            logger.info(f\"build_jdbc_url - database : {database}\")\r\n            url = (\r\n                f\"jdbc:sqlserver://{host}:{port};\"\r\n                f\"database={database};\"\r\n                \"encrypt=true;\"\r\n                \"trustServerCertificate=false;\"\r\n                \"loginTimeout=30;\"\r\n            )\r\n        else:\r\n            url = f\"jdbc:sqlserver://{host}:{port}\"\r\n        driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n\r\n    elif st == \"postgresql\":\r\n        db = secret.get(\"database\") or secret.get(\"dbname\") or \"postgres\"\r\n        url = f\"jdbc:postgresql://{host}:{port}/{db}\"\r\n        driver = \"org.postgresql.Driver\"\r\n        logger.info(f\"build_jdbc_url - using database: {db}\")\r\n\r\n    elif st == \"mysql\":\r\n        url = f\"jdbc:mysql://{host}:{port}\"\r\n        driver = \"com.mysql.jdbc.Driver\"\r\n\r\n    elif st == \"oracle\":\r\n        svc_name = (\r\n            secret.get(\"service_name\")\r\n            or secret.get(\"SERVICE_NAME\")\r\n            or secret.get(\"servicename\")\r\n            or secret.get(\"SERVICE\")\r\n            or secret.get(\"service\")\r\n            or secret.get(\"serviceName\")\r\n        )\r\n        sid = secret.get(\"sid\") or secret.get(\"SID\")\r\n        connect_descriptor = (\r\n            secret.get(\"connect_descriptor\") or secret.get(\"CONNECT_DESCRIPTOR\")\r\n        )\r\n        ezconnect = secret.get(\"ezconnect\") or secret.get(\"EZCONNECT\")\r\n        if svc_name:\r\n            url = f\"jdbc:oracle:thin:@//{host}:{port}/{svc_name}\"\r\n        elif sid:\r\n            url = f\"jdbc:oracle:thin:@{host}:{port}:{sid}\"\r\n        elif ezconnect:\r\n            url = f\"jdbc:oracle:thin:@//{ezconnect}\"\r\n        elif connect_descriptor:\r\n            url = f\"jdbc:oracle:thin:@{connect_descriptor}\"\r\n        else:\r\n            present = \", \".join(sorted(k for k in secret.keys()))\r\n            raise ValueError(\r\n                \"Oracle secret must include 'service_name' or 'sid' \"\r\n                f\"(present keys: {present})\"\r\n            )\r\n        driver = \"oracle.jdbc.OracleDriver\"\r\n\r\n    elif st == \"redshift\":\r\n        db = secret.get(\"database\") or secret.get(\"dbname\") or \"\"\r\n        path = f\"/{db}\" if db else \"\"\r\n        params = secret.get(\"jdbc_params\") or {}\r\n        if params:\r\n            query = \"&\".join(f\"{k}={v}\" for k, v in params.items())\r\n            url = f\"jdbc:redshift://{host}:{port}{path}?{query}\"\r\n        else:\r\n            url = f\"jdbc:redshift://{host}:{port}{path}\"\r\n        driver = secret.get(\"driver\", \"com.amazon.redshift.jdbc.Driver\")\r\n\r\n    elif st in (\"azuresql\", \"sql\", \"mssql\", \"azure_sql\"):\r\n        database = secret.get(\"database\")\r\n        if not database:\r\n            raise ValueError(\r\n                \"Azure SQL secret must include 'database' key \"\r\n                \"(e.g. DataFramework) in AWS Secrets Manager.\"\r\n            )\r\n\r\n        logger.info(f\"build_jdbc_url - database : {database}\")\r\n\r\n        url = (\r\n            f\"jdbc:sqlserver://{host}:{port};\"\r\n            f\"database={database};\"\r\n            \"encrypt=true;\"\r\n            \"trustServerCertificate=false;\"\r\n            \"loginTimeout=30;\"\r\n        )\r\n        driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\r\n\r\n    elif st == \"saphana\":\r\n        url = secret.get(\"url\") or f\"jdbc:sap://{host}:{port}\"\r\n        params = secret.get(\"jdbc_params\") or {}\r\n        if secret.get(\"currentschema\"):\r\n            params.setdefault(\"currentSchema\", secret.get(\"currentschema\"))\r\n        if params:\r\n            def _v(v):\r\n                return str(v).lower() if isinstance(v, bool) else v\r\n\r\n            query = \"&\".join(f\"{k}={_v(v)}\" for k, v in params.items())\r\n            sep = \"&\" if \"?\" in url else \"/?\"\r\n            url = f\"{url}{sep}{query}\"\r\n        driver = secret.get(\"driver\", \"com.sap.db.jdbc.Driver\")\r\n\r\n    else:\r\n        logger.error(f\"Unsupported JDBC source type: {source_type}\")\r\n        raise ValueError(f\"Unsupported JDBC source type: {source_type}\")\r\n\r\n    logger.info(f\"Constructed JDBC URL: {url} with driver: {driver}\")\r\n    return url, driver\r\n\r\n\r\n# ===== BRONZE LAYER LOADING SECTION =====\r\ndef load_bronze(\r\n    spark, sf_options, secret, md, dedup_df=None, keys=None, stage_suffix=\"_STAGE\"\r\n):\r\n    load_type = md[\"pipeline\"][\"LOAD_TYPE\"].strip().lower()\r\n    dest_table = md[\"destination\"][\"DESTINATIONPATH\"]\r\n    custom_query = md[\"pipeline\"].get(\"CUSTOMQUERY\")\r\n    if custom_query and str(custom_query).strip().upper() in (\"NULL\", \"NONE\", \"\"):\r\n        custom_query = None\r\n\r\n    logger.info(f\"{load_type} → Starting RAW load into table: {dest_table}\")\r\n\r\n    table_exists = table_exists_in_snowflake(secret, dest_table)\r\n    logger.info(f\"Table exists? {table_exists}\")\r\n\r\n    try:\r\n        if table_exists:\r\n            before = spark_snowflake_retry(\r\n                lambda: spark.read.format(\"snowflake\")\r\n                .options(**sf_options)\r\n                .option(\"query\", f\"SELECT COUNT(1) AS COUNT FROM {dest_table}\")\r\n                .load()\r\n                .collect()[0][\"COUNT\"],\r\n                action_desc=\"get row count\",\r\n            )\r\n            logger.info(f\"Previous row count in {dest_table}: {before}\")\r\n        else:\r\n            before = 0\r\n            logger.warning(f\"{dest_table} does not exist yet. Assuming 0 rows.\")\r\n    except Exception as e:\r\n        before = 0\r\n        logger.warning(\r\n            f\"Unable to get row count from {dest_table}. Assuming 0. Error: {str(e)}\"\r\n        )\r\n\r\n    if custom_query:\r\n        logger.info(\"Using CustomQuery to load RAW table.\")\r\n        data_df = spark_snowflake_retry(\r\n            lambda: spark.read.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"query\", custom_query)\r\n            .load(),\r\n            action_desc=\"read custom query\",\r\n        )\r\n        cols = data_df.columns\r\n    else:\r\n        if dedup_df is None:\r\n            logger.error(\"dedup_df is required if no CustomQuery is provided.\")\r\n            raise ValueError(\"dedup_df is required if no CustomQuery is provided\")\r\n        data_df = dedup_df\r\n        cols = [m[\"DESTINATIONCOLUMNNAME\"] for m in md[\"mappings\"]]\r\n\r\n    logger.debug(f\"Columns used for loading: {', '.join(cols)}\")\r\n\r\n    if not table_exists:\r\n        if custom_query:\r\n            logger.info(\"Creating table using schema from custom query.\")\r\n            ddl_cols = \", \".join(f\"{c} STRING\" for c in cols)\r\n        else:\r\n            ddl_cols = \", \".join(\r\n                f\"{m['DESTINATIONCOLUMNNAME']} {m['DESTINATIONCOLUMNDATATYPE']}\"\r\n                for m in sorted(\r\n                    md[\"mappings\"],\r\n                    key=lambda x: x.get(\"DESTINATIONCOLUMNSEQUENCE\", 0),\r\n                )\r\n            )\r\n        create_sql = f\"CREATE TABLE {dest_table} ({ddl_cols})\"\r\n        logger.info(f\"Creating table using SQL: {create_sql}\")\r\n        execute_snowflake_sql(secret, create_sql)\r\n\r\n    try:\r\n        if load_type == \"full\":\r\n            logger.info(\"Performing FULL load → Truncate + Insert\")\r\n            execute_snowflake_sql(secret, f\"TRUNCATE TABLE {dest_table}\")\r\n            spark_snowflake_retry(\r\n                lambda: data_df.write.mode(\"append\")\r\n                .format(\"snowflake\")\r\n                .options(**sf_options)\r\n                .option(\"dbtable\", dest_table)\r\n                .save(),\r\n                action_desc=\"full load write\",\r\n            )\r\n\r\n        elif load_type == \"append\":\r\n            logger.info(\r\n                \"Performing APPEND load → Insert only new records based on watermark\"\r\n            )\r\n            watermark_col = md[\"pipeline\"].get(\"WATERMARKCOLUMN\") or md[\"pipeline\"].get(\r\n                \"WATERMARK_COLUMN\"\r\n            )\r\n            max_wm = None\r\n\r\n            if watermark_col is None:\r\n                logger.warning(\r\n                    \"No WATERMARK_COLUMN defined in pipeline metadata. Appending all records.\"\r\n                )\r\n            else:\r\n                if table_exists:\r\n                    try:\r\n                        wm_df = spark_snowflake_retry(\r\n                            lambda: spark.read.format(\"snowflake\")\r\n                            .options(**sf_options)\r\n                            .option(\r\n                                \"query\",\r\n                                f\"SELECT MAX({watermark_col}) AS max_wm FROM {dest_table}\",\r\n                            )\r\n                            .load(),\r\n                            action_desc=\"max watermark read\",\r\n                        )\r\n                        if wm_df.count() > 0 and wm_df.first()[\"MAX_WM\"] is not None:\r\n                            max_wm = wm_df.first()[\"MAX_WM\"]\r\n                            logger.info(\r\n                                f\"Max watermark value in destination table for column {watermark_col}: {max_wm}\"\r\n                            )\r\n                    except Exception as e:\r\n                        logger.warning(\r\n                            f\"Could not fetch MAX({watermark_col}) from {dest_table}: {str(e)}\"\r\n                        )\r\n\r\n            if max_wm is not None:\r\n                logger.info(f\"Filtering source data where {watermark_col} > {max_wm}\")\r\n                data_df = data_df.filter(col(watermark_col) > lit(max_wm))\r\n            else:\r\n                logger.info(\"No watermark found or no previous data. Loading all source records.\")\r\n\r\n            if data_df.rdd.isEmpty():\r\n                logger.info(\"No new records\")\r\n                return dest_table, before\r\n\r\n            spark_snowflake_retry(\r\n                lambda: data_df.write.mode(\"append\")\r\n                .format(\"snowflake\")\r\n                .options(**sf_options)\r\n                .option(\"dbtable\", dest_table)\r\n                .save(),\r\n                action_desc=\"append load write\",\r\n            )\r\n\r\n        elif load_type == \"incremental\":\r\n            logger.info(\"Performing INCREMENTAL load → MERGE based on primary key\")\r\n            if not keys:\r\n                logger.error(\"Primary keys must be provided for INCREMENTAL load\")\r\n                raise ValueError(\"Primary keys must be provided for INCREMENTAL load\")\r\n\r\n            watermark_col = md[\"pipeline\"].get(\"WATERMARKCOLUMN\") or md[\"pipeline\"].get(\r\n                \"WATERMARK_COLUMN\"\r\n            )\r\n            if watermark_col and table_exists:\r\n                try:\r\n                    wm_df = spark_snowflake_retry(\r\n                        lambda: spark.read.format(\"snowflake\")\r\n                        .options(**sf_options)\r\n                        .option(\r\n                            \"query\",\r\n                            f\"SELECT MAX({watermark_col}) AS max_wm FROM {dest_table}\",\r\n                        )\r\n                        .load(),\r\n                        action_desc=\"max watermark read (incremental)\",\r\n                    )\r\n                    dest_max_wm = wm_df.first()[\"MAX_WM\"] if wm_df.count() > 0 else None\r\n                    if dest_max_wm is not None:\r\n                        data_df = data_df.filter(col(watermark_col) > lit(dest_max_wm))\r\n                        if data_df.rdd.isEmpty():\r\n                            logger.info(\"No new records\")\r\n                            return dest_table, before\r\n                except Exception as e:\r\n                    logger.warning(\r\n                        f\"Could not fetch MAX({watermark_col}) from {dest_table}: {str(e)}\"\r\n                    )\r\n\r\n            stage_table = dest_table + stage_suffix\r\n            logger.info(\r\n                f\"Writing deduped DataFrame to Snowflake staging table: {stage_table}\"\r\n            )\r\n            spark_snowflake_retry(\r\n                lambda: data_df.write.mode(\"overwrite\")\r\n                .format(\"snowflake\")\r\n                .options(**sf_options)\r\n                .option(\"dbtable\", stage_table)\r\n                .save(),\r\n                action_desc=\"incremental stage write\",\r\n            )\r\n\r\n            update_clause = \", \".join(\r\n                [f\"target.{c}=source.{c}\" for c in cols if c not in keys]\r\n            )\r\n            insert_cols = \", \".join(cols)\r\n            insert_vals = \", \".join([f\"source.{c}\" for c in cols])\r\n            on_clause = \" AND \".join([f\"target.{k}=source.{k}\" for k in keys])\r\n\r\n            merge_sql = f\"\"\"\r\n                MERGE INTO {dest_table} AS target\r\n                USING {stage_table} AS source\r\n                ON {on_clause}\r\n                WHEN MATCHED THEN UPDATE SET {update_clause}\r\n                WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\r\n            \"\"\"\r\n            logger.debug(f\"Executing merge SQL: {merge_sql.strip()}\")\r\n            execute_snowflake_sql(secret, merge_sql)\r\n\r\n            try:\r\n                logger.info(\r\n                    f\"Truncating staging table {stage_table} to save storage.\"\r\n                )\r\n                execute_snowflake_sql(secret, f\"TRUNCATE TABLE {stage_table}\")\r\n            except Exception as e:\r\n                logger.warning(\r\n                    f\"Failed to truncate staging table {stage_table}: {str(e)}\"\r\n                )\r\n\r\n        else:\r\n            logger.error(f\"Unsupported LoadType: {load_type}\")\r\n            raise ValueError(f\"Unsupported LoadType: {load_type}\")\r\n\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"RAW load failed for {dest_table}: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n    logger.info(\r\n        f\"Completed RAW load → {dest_table}. Previous Row Count: {before}\"\r\n    )\r\n    return dest_table, before\r\n\r\n\r\n# ===== DATA INGESTION LOGGING SECTION =====\r\ndef log_data_ingestion(\r\n    spark,\r\n    sf_options,\r\n    master_log_id,\r\n    status_message_id,\r\n    pipeline_id,\r\n    layer,\r\n    trigger_type=\"SCHEDULED\",\r\n    pipeline_start_time=None,\r\n    pipeline_end_time=None,\r\n    pipeline_status=None,\r\n    errortype=None,\r\n    error_message=None,\r\n    error_severity=None,\r\n    error_datetime=None,\r\n    source_count=0,\r\n    destination_count=0,\r\n    inserted_count=0,\r\n    updated_count=0,\r\n    deleted_count=0,\r\n    log_table_name=None,\r\n):\r\n    \"\"\"\r\n    Build a DataFrame that EXACTLY matches CONFIG.LOG table (23 columns, exact order):\r\n      LOGID, PIPELINEID, PIPELINESTARTDATETIME, PIPELINEENDDATETIME, PIPELINESTATUS,\r\n      STATUSMESSAGEID, MASTERLOGID, INSTANCEID, INSTANCESTARTDATETIME, INSTANCEENDDATETIME,\r\n      INSTANCESTATUS, ERRORMESSAGE, ERRORSEVERITY, ERRORDATETIME, SOURCECOUNT, DESTINATIONCOUNT,\r\n      INSERTEDCOUNT, UPDATEDCOUNT, DELETEDCOUNT, TOTALINSTANCECOUNT, TRIGGERTYPE, LOGDATETIME, LAYER\r\n    \"\"\"\r\n    log_id = str(uuid.uuid4())\r\n    current_time = datetime.now(timezone.utc)\r\n\r\n    instance_id = None\r\n    instance_start = None\r\n    instance_end = None\r\n    instance_status = None\r\n    total_instance_count = 1\r\n\r\n    schema = StructType(\r\n        [\r\n            StructField(\"LOGID\", StringType(), False),\r\n            StructField(\"PIPELINEID\", StringType(), False),\r\n            StructField(\"PIPELINESTARTDATETIME\", TimestampType(), True),\r\n            StructField(\"PIPELINEENDDATETIME\", TimestampType(), True),\r\n            StructField(\"PIPELINESTATUS\", StringType(), True),\r\n            StructField(\"STATUSMESSAGEID\", LongType(), False),\r\n            StructField(\"MASTERLOGID\", StringType(), False),\r\n            StructField(\"INSTANCEID\", StringType(), True),\r\n            StructField(\"INSTANCESTARTDATETIME\", TimestampType(), True),\r\n            StructField(\"INSTANCEENDDATETIME\", TimestampType(), True),\r\n            StructField(\"INSTANCESTATUS\", StringType(), True),\r\n            StructField(\"ERRORMESSAGE\", StringType(), True),\r\n            StructField(\"ERRORSEVERITY\", StringType(), True),\r\n            StructField(\"ERRORDATETIME\", TimestampType(), True),\r\n            StructField(\"SOURCECOUNT\", IntegerType(), True),\r\n            StructField(\"DESTINATIONCOUNT\", IntegerType(), True),\r\n            StructField(\"INSERTEDCOUNT\", IntegerType(), True),\r\n            StructField(\"UPDATEDCOUNT\", IntegerType(), True),\r\n            StructField(\"DELETEDCOUNT\", IntegerType(), True),\r\n            StructField(\"TOTALINSTANCECOUNT\", IntegerType(), True),\r\n            StructField(\"TRIGGERTYPE\", StringType(), True),\r\n            StructField(\"LOGDATETIME\", TimestampType(), False),\r\n            StructField(\"LAYER\", StringType(), True),\r\n        ]\r\n    )\r\n\r\n    values = [\r\n        (\r\n            log_id,\r\n            pipeline_id,\r\n            pipeline_start_time,\r\n            pipeline_end_time,\r\n            pipeline_status,\r\n            int(status_message_id),\r\n            master_log_id,\r\n            instance_id,\r\n            instance_start,\r\n            instance_end,\r\n            instance_status,\r\n            error_message,\r\n            error_severity,\r\n            error_datetime if error_message else None,\r\n            int(source_count),\r\n            int(destination_count),\r\n            int(inserted_count),\r\n            int(updated_count),\r\n            int(deleted_count),\r\n            int(total_instance_count),\r\n            trigger_type,\r\n            current_time,\r\n            layer,\r\n        )\r\n    ]\r\n\r\n    log_df = spark.createDataFrame(values, schema=schema)\r\n\r\n    spark_snowflake_retry(\r\n        lambda: log_df.write.format(\"snowflake\")\r\n        .options(**sf_options)\r\n        .option(\"dbtable\", log_table_name)\r\n        .mode(\"append\")\r\n        .save(),\r\n        action_desc=\"data ingestion log\",\r\n    )\r\n\r\n    print(\r\n        f\"Log written to Snowflake: {pipeline_status} with counts – Source: {source_count}, Dest: {destination_count}\"\r\n    )\r\n\r\n\r\n# ===== MASTER LOG FINALIZATION =====\r\ndef finalize_master_log(secret, master_log_id, master_status, message=None, table_name=None):\r\n    try:\r\n        logger.info(\r\n            f\"Finalizing MASTERLOG ID {master_log_id} with status → {master_status}\"\r\n        )\r\n\r\n        end_dt_str = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\r\n        update_sql = (\r\n            f\"UPDATE {table_name} \"\r\n            f\"SET ENDDATETIME = TO_TIMESTAMP('{end_dt_str}'), \"\r\n            f\"STATUS = '{master_status}' \"\r\n            f\"WHERE MASTERLOGID = '{master_log_id}'\"\r\n        )\r\n\r\n        logger.info(f\"Executing update SQL:\\n{update_sql}\")\r\n        execute_snowflake_sql(secret, update_sql)\r\n        logger.info(\r\n            f\"MASTERLOG ID {master_log_id} successfully updated with status → {master_status}\"\r\n        )\r\n\r\n    except AnalysisException as ae:\r\n        logger.error(\r\n            f\"Snowflake AnalysisException while updating MASTERLOG ID {master_log_id}: {ae}\"\r\n        )\r\n        raise\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Unexpected error while finalizing MASTERLOG ID {master_log_id}: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n\r\n# ===== CRON UPDATE SECTION =====\r\ndef update_cron_next_run(secret, cron_id, cron_expr, tz, table_name):\r\n    \"\"\"\r\n    Update NEXTRUN in CRON table by CRONID (no PIPELINEID in CRON anymore).\r\n    \"\"\"\r\n    now = datetime.now(pytz.timezone(tz))\r\n    iter_obj = croniter(cron_expr, now)\r\n    next_run = iter_obj.get_next(datetime)\r\n    next_run_utc = next_run.astimezone(pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\r\n\r\n    update_sql = f\"\"\"\r\n        UPDATE {table_name}\r\n        SET NEXTRUN = TO_TIMESTAMP('{next_run_utc}')\r\n        WHERE CRONID = '{cron_id}'\r\n    \"\"\"\r\n    execute_snowflake_sql(secret, update_sql)\r\n    logger.info(f\"Updated NEXTRUN for CRONID={cron_id} to {next_run_utc}\")\r\n\r\n\r\ndef update_cron_last_run(secret, cron_id, table_name):\r\n    \"\"\"\r\n    Update LASTRUN in CRON table by CRONID (no PIPELINEID in CRON anymore).\r\n    \"\"\"\r\n    now_utc = datetime.now(pytz.UTC).strftime(\"%Y-%m-%d %H:%M:%S\")\r\n    update_sql = f\"\"\"\r\n        UPDATE {table_name}\r\n        SET LASTRUN = TO_TIMESTAMP('{now_utc}')\r\n        WHERE CRONID = '{cron_id}'\r\n    \"\"\"\r\n    execute_snowflake_sql(secret, update_sql)\r\n    logger.info(f\"Updated LASTRUN for CRONID={cron_id} to {now_utc}\")\r\n\r\n\r\ndef api_request(\r\n    url,\r\n    method=\"GET\",\r\n    authType=None,\r\n    Token=None,\r\n    headers=None,\r\n    payload=None,\r\n    params=None,\r\n    verify=True,\r\n    raise_for_status=False,\r\n    timeout=30,\r\n):\r\n    method = method.upper()\r\n    session = requests.Session()\r\n\r\n    hdrs = {} if headers is None else dict(headers)\r\n\r\n    if payload is not None and \"Content-Type\" not in {k.title(): v for k, v in hdrs.items()}:\r\n        hdrs.setdefault(\"Content-Type\", \"application/json\")\r\n\r\n    if authType:\r\n        a = authType.upper()\r\n        if a == \"BASIC\" and isinstance(Token, (tuple, list)) and len(Token) == 2:\r\n            session.auth = HTTPBasicAuth(Token[0], Token[1])\r\n        elif a == \"BEARER\" and isinstance(Token, str):\r\n            hdrs.setdefault(\"Authorization\", f\"Bearer {Token}\")\r\n        elif a == \"JWT\" and isinstance(Token, str):\r\n            hdrs.setdefault(\"Authorization\", f\"JWT {Token}\")\r\n        elif a == \"API_KEY\" and isinstance(Token, str):\r\n            hdrs.setdefault(\"x-api-key\", Token)\r\n\r\n    session.headers.update(hdrs)\r\n\r\n    try:\r\n        if method == \"GET\":\r\n            response = session.get(url, params=params, timeout=timeout, verify=verify)\r\n        elif method == \"POST\":\r\n            response = session.post(\r\n                url, json=payload, params=params, timeout=timeout, verify=verify\r\n            )\r\n        elif method == \"PUT\":\r\n            response = session.put(\r\n                url, json=payload, params=params, timeout=timeout, verify=verify\r\n            )\r\n        elif method == \"DELETE\":\r\n            response = session.delete(url, params=params, timeout=timeout, verify=verify)\r\n        else:\r\n            raise ValueError(f\"Unsupported HTTP method: {method}\")\r\n\r\n        if raise_for_status:\r\n            response.raise_for_status()\r\n\r\n        try:\r\n            body = response.json()\r\n        except ValueError:\r\n            body = response.text\r\n\r\n        return {\r\n            \"status_code\": response.status_code,\r\n            \"headers\": dict(response.headers),\r\n            \"body\": body,\r\n            \"request_headers\": dict(response.request.headers),\r\n            \"url\": response.url,\r\n        }\r\n    except requests.exceptions.RequestException as e:\r\n        return {\"error\": str(e)}\r\n\r\n\r\ndef extract_token_from_response(resp_json):\r\n    \"\"\"\r\n    flexibly extract a token string from resp_json which may be:\r\n      - dict containing common keys\r\n      - a plain string\r\n      - wrapper dict containing 'body' which is a dict\r\n    \"\"\"\r\n    if isinstance(resp_json, dict) and \"body\" in resp_json:\r\n        body = resp_json.get(\"body\")\r\n    else:\r\n        body = resp_json\r\n\r\n    if isinstance(body, dict):\r\n        for k in (\r\n            \"generatedToken\",\r\n            \"token\",\r\n            \"jwt\",\r\n            \"access_token\",\r\n            \"accessToken\",\r\n            \"Token\",\r\n        ):\r\n            v = body.get(k)\r\n            if isinstance(v, str) and len(v) > 0:\r\n                return v\r\n\r\n        for v in body.values():\r\n            if isinstance(v, str) and len(v) > 20:\r\n                return v\r\n\r\n    if isinstance(body, str) and len(body) > 0:\r\n        return body\r\n\r\n    return None\r\n\r\n\r\ndef put_generate_token(\r\n    url,\r\n    source_system_name,\r\n    headers=None,\r\n    max_retries=None,\r\n    backoff_seconds=None,\r\n    timeout=None,\r\n    return_full_response_on_failure=False,\r\n):\r\n    payload = {\"sourceSystemName\": source_system_name}\r\n    attempt = 0\r\n    last_resp = None\r\n\r\n    max_retries = int(\r\n        max_retries if max_retries is not None else DEFAULTS.get(\"DEFAULT_MAX_RETRIES\", 3)\r\n    )\r\n    backoff_seconds = int(\r\n        backoff_seconds\r\n        if backoff_seconds is not None\r\n        else DEFAULTS.get(\"DEFAULT_RETRY_DELAY_SECONDS\", 60)\r\n    )\r\n    timeout = int(\r\n        timeout if timeout is not None else DEFAULTS.get(\"DEFAULT_TIMEOUT\", 30)\r\n    )\r\n\r\n    while attempt < max_retries:\r\n        resp = api_request(\r\n            url,\r\n            method=\"PUT\",\r\n            authType=None,\r\n            Token=None,\r\n            headers=headers,\r\n            payload=payload,\r\n            params=None,\r\n            timeout=timeout,\r\n        )\r\n        if isinstance(resp, dict) and resp.get(\"error\"):\r\n            logger.warning(\r\n                \"Attempt %d: api_request error generating token for %s: %s\",\r\n                attempt + 1,\r\n                source_system_name,\r\n                resp.get(\"error\"),\r\n            )\r\n            last_resp = resp\r\n        else:\r\n            status = resp.get(\"status_code\")\r\n            last_resp = resp\r\n            if status in (200, 201):\r\n                token = extract_token_from_response(resp)\r\n                if token:\r\n                    logger.info(\r\n                        \"Generated token for %s (len=%d)\",\r\n                        source_system_name,\r\n                        len(token),\r\n                    )\r\n                    return token\r\n                else:\r\n                    logger.warning(\r\n                        \"200/201 returned but no token found in body for %s. body-type=%s\",\r\n                        source_system_name,\r\n                        type(resp.get(\"body\")),\r\n                    )\r\n                    if return_full_response_on_failure:\r\n                        return None, resp\r\n                    return None\r\n            elif status == 204:\r\n                logger.warning(\r\n                    \"204 No Content returned for %s - check API behaviour\",\r\n                    source_system_name,\r\n                )\r\n                if return_full_response_on_failure:\r\n                    return None, resp\r\n                return None\r\n            else:\r\n                if 400 <= (status or 0) < 500 and status != 429:\r\n                    logger.error(\r\n                        \"Client error while generating token for %s: status=%s, body=%s\",\r\n                        source_system_name,\r\n                        status,\r\n                        resp.get(\"body\"),\r\n                    )\r\n                    if return_full_response_on_failure:\r\n                        return None, resp\r\n                    return None\r\n                logger.warning(\r\n                    \"Transient API response for %s: status=%s. Will retry.\",\r\n                    status,\r\n                )\r\n\r\n        attempt += 1\r\n        sleep_seconds = backoff_seconds * attempt\r\n        logger.info(\r\n            \"Waiting %d seconds before retrying token generation (attempt %d/%d) for %s\",\r\n            sleep_seconds,\r\n            attempt,\r\n            max_retries,\r\n            source_system_name,\r\n        )\r\n        time.sleep(sleep_seconds)\r\n\r\n    logger.error(\"Failed to generate token for %s after %d attempts\", source_system_name, max_retries)\r\n    if return_full_response_on_failure and last_resp is not None:\r\n        return None, last_resp\r\n    return None\r\n\r\n\r\ndef to_rfc3339_z(dt):\r\n    \"\"\"\r\n    Convert datetime or ISO string to RFC3339 UTC string ending with 'Z'.\r\n    - If dt is None -> returns None\r\n    - If dt is a datetime (naive -> treated as UTC) -> returns 'YYYY-MM-DDTHH:MM:SS.ssssssZ'\r\n    - If dt is a string that already ends with '+00:00Z' or '+00:00', normalize to Z-form.\r\n    \"\"\"\r\n    if dt is None:\r\n        return None\r\n    if isinstance(dt, str):\r\n        s = dt\r\n        if s.endswith(\"+00:00Z\"):\r\n            s = s.replace(\"+00:00Z\", \"Z\")\r\n        if s.endswith(\"+00:00\"):\r\n            s = s.replace(\"+00:00\", \"Z\")\r\n        return s\r\n    if dt.tzinfo is None:\r\n        dt = dt.replace(tzinfo=timezone.utc)\r\n    return dt.astimezone(timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\r\n\r\n\r\ndef build_globalconfig_payload(\r\n    md, job_id=None, start_ts=None, end_ts=None, record_count=None, status=\"C\"\r\n):\r\n    \"\"\"Build the JSON model expected by GlobalConfig API using md returned by load_metadata.\"\"\"\r\n    now_dt = datetime.utcnow().replace(tzinfo=timezone.utc)\r\n    now = to_rfc3339_z(now_dt)\r\n    start_str = to_rfc3339_z(start_ts) if start_ts else to_rfc3339_z(now_dt)\r\n    end_str = to_rfc3339_z(end_ts) if end_ts else to_rfc3339_z(now_dt)\r\n\r\n    source_name = (\r\n        DEFAULTS.get(\"sourceSystemName\")\r\n        or md[\"source\"].get(\"SOURCENAME\")\r\n        or \"Snowflake_DE\"\r\n    )\r\n    job_id = job_id[:45] if job_id else str(uuid.uuid4())\r\n\r\n    tpath = md[\"destination\"].get(\"DESTINATIONPATH\") or \"\"\r\n    tparts = tpath.split(\".\")\r\n    if len(tparts) >= 3:\r\n        target_schema, target_table = tparts[-2], tparts[-1]\r\n    elif len(tparts) == 2:\r\n        target_schema, target_table = tparts[0], tparts[1]\r\n    else:\r\n        target_schema, target_table = \"\", \"\"\r\n\r\n    s_path = md[\"source\"].get(\"SOURCEPATH\") or \"\"\r\n\r\n    if \"/\" in s_path:\r\n        source_table = s_path.split(\"/\")[-1]\r\n        source_schema = None\r\n\r\n    elif \".\" in s_path:\r\n        parts = s_path.split(\".\")\r\n        source_table = parts[-1]\r\n        source_schema = \".\".join(parts[:-1])\r\n    else:\r\n        source_table = s_path\r\n        source_schema = None\r\n\r\n    job_config = {\r\n        \"jobIDDetails\": job_id,\r\n        \"sourceSystemName\": source_name,\r\n        \"targetSystemName\": md[\"destination\"].get(\"DESTINATIONNAME\") or \"Snowflake\",\r\n        \"description\": md[\"pipeline\"].get(\"PIPELINENAME\")\r\n        or md[\"pipeline\"].get(\"DESCRIPTION\")\r\n        or \"\",\r\n        \"criticalFlag\": 4,\r\n        \"severity\": 1,\r\n        \"completedAlert\": \"Y\",\r\n        \"failedAlert\": \"Y\",\r\n        \"delayedAlert\": \"Y\",\r\n        \"validationlAlert\": \"N\",\r\n        \"sourceSystemLocation\": s_path,\r\n        \"targetSystemLocation\": md[\"destination\"].get(\"DESTINATIONPATH\"),\r\n        \"sourceSchemaName\": source_schema,\r\n        \"sourceTableName\": source_table,\r\n        \"targetSchemaName\": target_schema,\r\n        \"targetTableName\": target_table,\r\n        \"operationType\": md[\"pipeline\"].get(\"LOAD_TYPE\") or \"ETL\",\r\n        \"isBusinessAlert\": \"Y\",\r\n        \"businessAlertEmail\": DEFAULTS.get(\"BusinessAlertEmail\", None),\r\n        \"helpDeskConfigID\": 1,\r\n        \"runSchedule\": \"Y\",\r\n        \"day\": \"Daily\",\r\n        \"time\": datetime.utcnow().strftime(\"%H:%M\"),\r\n        \"estimatedTime\": (\r\n            start_ts.strftime(\"%H:%M:%S\")\r\n            if isinstance(start_ts, datetime)\r\n            else (md.get(\"pipeline\", {}).get(\"ESTIMATEDTIME\") or None)\r\n        ),\r\n        \"isEnabled\": \"S\",\r\n        \"insertedDateTime\": now,\r\n        \"importedDateTime\": now,\r\n        \"sourceAlertLog\": [\r\n            {\r\n                \"jobIDDetails\": job_id,\r\n                \"uniqueJobIDReference\": job_id,\r\n                \"sourceSystemName\": source_name,\r\n                \"transactionStatus\": status,\r\n                \"jobStatus\": status,\r\n                \"reportedDateTime\": end_str,\r\n                \"sourceRecordCount\": int(record_count or 0),\r\n                \"startDateTime\": start_str,\r\n                \"endDateTime\": end_str,\r\n                \"message\": f\"{job_id}_{source_name}\",\r\n                \"pF1\": \"success\" if status == \"C\" else \"failed\",\r\n                \"pF2\": str(record_count or 0),\r\n                \"pF3\": \"automated\",\r\n            }\r\n        ],\r\n    }\r\n\r\n    source_config = {\r\n        \"sourceSystemName\": source_name,\r\n        \"sourceType\": md[\"source\"].get(\"SOURCE_TYPE\", \"Snowflake\"),\r\n        \"description\": (md.get(\"source\", {}).get(\"SOURCEDESCRIPTION\") or \"\")\r\n        .strip()\r\n        or \"Snowflake data framework\",\r\n        \"helpdeskAlert\": \"Y\",\r\n        \"hostAddress\": \"string\",\r\n        \"connectionConfig\": \"string\",\r\n        \"databaseType\": DEFAULTS.get(\"CONFIG_DATABASE\", None),\r\n        \"isEnabled\": \"A\",\r\n        \"insertedDateTime\": now,\r\n        \"importedDateTime\": now,\r\n    }\r\n\r\n    payload = {\"sourceConfig\": source_config, \"sourceJobConfig\": [job_config]}\r\n    return payload\r\n\r\n\r\ndef insert_global_config(\r\n    master_log_id,\r\n    md,\r\n    job_id,\r\n    start_ts,\r\n    end_ts,\r\n    record_count,\r\n    status,\r\n    token=None,\r\n    max_retries=None,\r\n    backoff_seconds=None,\r\n    timeout=None,\r\n    return_full_response_on_failure=False,\r\n):\r\n    payload = build_globalconfig_payload(\r\n        md,\r\n        job_id=job_id,\r\n        start_ts=start_ts,\r\n        end_ts=end_ts,\r\n        record_count=record_count,\r\n        status=status,\r\n    )\r\n\r\n    hdrs = {\"Content-Type\": \"application/json\"}\r\n    if token:\r\n        hdrs.setdefault(\"Authorization\", f\"Bearer {token}\")\r\n\r\n    max_retries = int(\r\n        max_retries if max_retries is not None else DEFAULTS.get(\"DEFAULT_MAX_RETRIES\", 3)\r\n    )\r\n    backoff_seconds = int(\r\n        backoff_seconds\r\n        if backoff_seconds is not None\r\n        else DEFAULTS.get(\"DEFAULT_RETRY_DELAY_SECONDS\", 60)\r\n    )\r\n    timeout = int(\r\n        timeout if timeout is not None else DEFAULTS.get(\"DEFAULT_TIMEOUT\", 30)\r\n    )\r\n\r\n    attempt = 0\r\n    last_resp = None\r\n\r\n    if token is None:\r\n        logger.error(\"No token - skipping GlobalConfig entry\")\r\n        return None\r\n\r\n    while attempt < max_retries:\r\n        resp = api_request(\r\n            url=DEFAULTS[\"GlobalAlerts_API_URL\"] + \"GlobalConfig\",\r\n            method=\"PUT\",\r\n            authType=\"Bearer\",\r\n            Token=token,\r\n            headers=hdrs,\r\n            payload=payload,\r\n            timeout=timeout,\r\n        )\r\n\r\n        if isinstance(resp, dict) and resp.get(\"error\"):\r\n            logger.warning(\r\n                \"Attempt %d: api_request error insert_global_config for %s: %s\",\r\n                attempt + 1,\r\n                md.get(\"source\", {}).get(\"SOURCENAME\", DEFAULTS[\"sourceSystemName\"]),\r\n                resp.get(\"error\"),\r\n            )\r\n            last_resp = resp\r\n        else:\r\n            status_code = resp.get(\"status_code\")\r\n            last_resp = resp\r\n            if status_code in (200, 201):\r\n                resp_body = resp.get(\"body\")\r\n                new_token = extract_token_from_response(resp)\r\n                if new_token:\r\n                    logger.info(\r\n                        \"insert_global_config returned token for %s (len=%d)\",\r\n                        md.get(\"source\", {}).get(\"SOURCENAME\", DEFAULTS[\"sourceSystemName\"]),\r\n                        len(new_token),\r\n                    )\r\n                    return new_token\r\n                return resp_body\r\n            elif status_code == 204:\r\n                logger.info(\"insert_global_config returned 204 No Content\")\r\n                return None\r\n            else:\r\n                if 400 <= (status_code or 0) < 500 and status_code != 429:\r\n                    logger.error(\r\n                        \"Client error in insert_global_config: status=%s body=%s\",\r\n                        status_code,\r\n                        resp.get(\"body\"),\r\n                    )\r\n                    if return_full_response_on_failure:\r\n                        return None, resp\r\n                    return None\r\n                logger.warning(\r\n                    \"Transient API response for insert_global_config: status=%s. Will retry.\",\r\n                    status_code,\r\n                )\r\n\r\n        attempt += 1\r\n        sleep_seconds = backoff_seconds * attempt\r\n        logger.info(\r\n            \"Waiting %d seconds before retrying insert_global_config (attempt %d/%d)\",\r\n            sleep_seconds,\r\n            attempt,\r\n            max_retries,\r\n        )\r\n        time.sleep(sleep_seconds)\r\n\r\n    logger.error(\"Failed insert_global_config after %d attempts\", max_retries)\r\n    if return_full_response_on_failure and last_resp is not None:\r\n        return None, last_resp\r\n    return None\r\n\r\n\r\n# ===== MAIN EXECUTION SECTION =====\r\ndef main():\r\n    logger.info(\"===== sf_de_framework ETL Job Started =====\")\r\n    spark = glueContext.spark_session\r\n\r\n    glueRunId = os.environ.get(\"AWS_GLUE_JOB_RUN_ID\") or f\"manual_run_{uuid.uuid4()}\"\r\n    logger.info(f\"Glue Job Run ID: {glueRunId}\")\r\n\r\n    try:\r\n        snowflake_secret = get_secret_cached(\r\n            \"sf_de_framework_snowflake_secret\",\r\n            region_name=DEFAULTS[\"SECRETS_REGION\"],\r\n        )\r\n        safe_secret = {**snowflake_secret, \"password\": \"***\"}\r\n        logger.info(\r\n            f\"Step 1: Fetched Snowflake Secret = {json.dumps(safe_secret)}\"\r\n        )\r\n\r\n        sf_options = create_snowflake_options(snowflake_secret)\r\n        sf_options_safe = {**sf_options, \"sfPassword\": \"***\"}\r\n        logger.info(\r\n            f\"Step 2: Snowflake options = {json.dumps(sf_options_safe)}\"\r\n        )\r\n\r\n        config_db_default = DEFAULTS[\"CONFIG_DATABASE\"]\r\n        config_schema_default = DEFAULTS[\"CONFIG_SCHEMA\"]\r\n\r\n        default_sysprops_fqn = f\"{config_db_default}.{config_schema_default}.SYSTEMPROPERTIES\"\r\n        sys_props = load_system_properties(spark, sf_options, default_sysprops_fqn)\r\n\r\n        CONFIG_DB = get_prop(sys_props, \"CONFIG_DATABASE\", config_db_default)\r\n        CONFIG_SCHEMA = get_prop(sys_props, \"CONFIG_SCHEMA\", config_schema_default)\r\n\r\n        def cfg(tname: str) -> str:\r\n            return f\"{CONFIG_DB}.{CONFIG_SCHEMA}.{tname}\"\r\n\r\n        logger.info(\r\n            f\"[Config] Using CONFIG schema → {CONFIG_DB}.{CONFIG_SCHEMA}\"\r\n        )\r\n\r\n        global DEFAULT_AWS_REGION\r\n        DEFAULT_AWS_REGION = get_prop(\r\n            sys_props, \"SECRETS_REGION\", DEFAULTS[\"SECRETS_REGION\"]\r\n        )\r\n\r\n        MASTERLOG_TABLE = get_prop(\r\n            sys_props,\r\n            \"MASTERLOG_TABLE\",\r\n            f\"{CONFIG_DB}.{CONFIG_SCHEMA}.MASTERLOG\",\r\n        )\r\n        LOG_TABLE = get_prop(\r\n            sys_props, \"LOG_TABLE\", f\"{CONFIG_DB}.{CONFIG_SCHEMA}.LOG\"\r\n        )\r\n        CRON_TABLE = get_prop(sys_props, \"CRON_TABLE\", f\"{CONFIG_DB}.{CONFIG_SCHEMA}.CRON\")\r\n\r\n        PIPELINE_SUCCESS_ID = int(\r\n            get_prop(sys_props, \"PIPELINE_SUCCESS_ID\", DEFAULTS[\"PIPELINE_SUCCESS_ID\"])\r\n        )\r\n        MASTER_SUCCESS_ID = int(\r\n            get_prop(sys_props, \"MASTER_SUCCESS_ID\", DEFAULTS[\"MASTER_SUCCESS_ID\"])\r\n        )\r\n        MASTER_FAILURE_ID = int(\r\n            get_prop(sys_props, \"MASTER_FAILURE_ID\", DEFAULTS[\"MASTER_FAILURE_ID\"])\r\n        )\r\n        STAGE_TABLE_SUFFIX = get_prop(\r\n            sys_props, \"STAGE_TABLE_SUFFIX\", DEFAULTS[\"STAGE_TABLE_SUFFIX\"]\r\n        )\r\n        DEFAULT_MAX_RETRIES = int(\r\n            get_prop(\r\n                sys_props,\r\n                \"DEFAULT_MAX_RETRIES\",\r\n                DEFAULTS[\"DEFAULT_MAX_RETRIES\"],\r\n            )\r\n        )\r\n\r\n        DEFAULT_RETRY_DELAY = int(\r\n            get_prop(\r\n                sys_props,\r\n                \"DEFAULT_RETRY_DELAY_SECONDS\",\r\n                DEFAULTS[\"DEFAULT_RETRY_DELAY_SECONDS\"],\r\n            )\r\n        )\r\n\r\n        logger.info(\r\n            f\"[Tables] MASTERLOG={MASTERLOG_TABLE}, LOG={LOG_TABLE}, CRON={CRON_TABLE}\"\r\n        )\r\n\r\n        status_df = spark_snowflake_retry(\r\n            lambda: spark.read.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"dbtable\", cfg(\"STATUSMESSAGE\"))\r\n            .load(),\r\n            action_desc=\"read STATUSMESSAGE\",\r\n        )\r\n        logger.info(\r\n            f\"Loaded STATUSMESSAGE table with {status_df.count()} rows\"\r\n        )\r\n\r\n        pipeline_df = spark_snowflake_retry(\r\n            lambda: spark.read.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"dbtable\", cfg(\"PIPELINE\"))\r\n            .load(),\r\n            action_desc=\"read PIPELINE\",\r\n        )\r\n        logger.info(\r\n            f\"Loaded PIPELINE table with {pipeline_df.count()} rows from {cfg('PIPELINE')}\"\r\n        )\r\n\r\n        cron_df = spark_snowflake_retry(\r\n            lambda: spark.read.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"dbtable\", CRON_TABLE)\r\n            .load(),\r\n            action_desc=\"read CRON\",\r\n        )\r\n        logger.info(\r\n            f\"Loaded CRON table with {cron_df.count()} rows from {CRON_TABLE}\"\r\n        )\r\n\r\n        lookup_df = spark_snowflake_retry(\r\n            lambda: spark.read.format(\"snowflake\")\r\n            .options(**sf_options)\r\n            .option(\"dbtable\", cfg(\"LOOKUP\"))\r\n            .load(),\r\n            action_desc=\"read LOOKUP\",\r\n        )\r\n        logger.info(\r\n            f\"Loaded LOOKUP table with {lookup_df.count()} rows from {cfg('LOOKUP')}\"\r\n        )\r\n\r\n        active_pipelines = (\r\n            pipeline_df.filter(\"ACTIVE = TRUE\")\r\n            .select(\"PIPELINEID\", \"LAYERID\", \"CRONID\")\r\n        )\r\n        cron_active = cron_df.filter(\"ACTIVE = TRUE\").select(\r\n            \"CRONID\", \"CRONEXPRESSION\", \"TIMEZONE\", \"NEXTRUN\"\r\n        )\r\n        joined_df = active_pipelines.join(cron_active, \"CRONID\")\r\n        logger.info(\r\n            \"Joined CRON and PIPELINE tables by CRONID; determining due pipelines.\"\r\n        )\r\n\r\n        due_pipelines = []\r\n        for row in joined_df.collect():\r\n            pid = row[\"PIPELINEID\"]\r\n            cron_id = row[\"CRONID\"]\r\n            expr = row[\"CRONEXPRESSION\"]\r\n            tz = row[\"TIMEZONE\"]\r\n            next_run = row[\"NEXTRUN\"]\r\n\r\n            now_tz = datetime.now(pytz.timezone(tz))\r\n            now_utc = now_tz.astimezone(pytz.UTC)\r\n            if next_run:\r\n                if next_run.tzinfo is None:\r\n                    next_utc = pytz.UTC.localize(next_run)\r\n                else:\r\n                    next_utc = next_run.astimezone(pytz.UTC)\r\n            else:\r\n                next_utc = None\r\n\r\n            if next_utc is None or now_utc >= next_utc:\r\n                due_pipelines.append((pid, cron_id, expr, tz))\r\n\r\n        if not due_pipelines:\r\n            logger.info(\"No pipelines are due for execution. Exiting main().\")\r\n            return\r\n        else:\r\n            endpoint_token = DEFAULTS[\"GlobalAlerts_API_URL\"] + \"JWTAuthentication\"\r\n            token = put_generate_token(\r\n                url=endpoint_token,\r\n                source_system_name=DEFAULTS[\"sourceSystemName\"],\r\n            )\r\n            if token:\r\n                logger.info(\r\n                    \"Token generated for %s (len=%d).\",\r\n                    DEFAULTS[\"sourceSystemName\"],\r\n                    len(token),\r\n                )\r\n            else:\r\n                logger.error(\r\n                    \"Token generation returned no token for %s; inspect API response/behavior\",\r\n                    DEFAULTS[\"sourceSystemName\"],\r\n                )\r\n\r\n        logger.info(\r\n            f\"Due pipelines: {[pid for pid, _, _, _ in due_pipelines]}\"\r\n        )\r\n\r\n        master_log_id = create_master_log(\r\n            spark,\r\n            sf_options,\r\n            glue_run_id=glueRunId,\r\n            total_pipelines=len(due_pipelines),\r\n            table_name=MASTERLOG_TABLE,\r\n        )\r\n        logger.info(f\"Master log created. MASTERLOG ID = {master_log_id}\")\r\n\r\n        any_failures = False\r\n\r\n        for pid, cron_id, expr, tz in due_pipelines:\r\n            update_cron_next_run(\r\n                snowflake_secret, cron_id, expr, tz, table_name=CRON_TABLE\r\n            )\r\n\r\n            max_retries, retrydelayseconds = get_retry_config_for_pipeline(\r\n                pipeline_df,\r\n                pid,\r\n                default_retries=DEFAULT_MAX_RETRIES,\r\n                default_delay=DEFAULT_RETRY_DELAY,\r\n            )\r\n            attempt = 0\r\n            while attempt < max_retries:\r\n                layer = \"UNKNOWN\"\r\n                try:\r\n                    logger.info(\r\n                        f\"----- [Pipeline Start] PIPELINEID: {pid}, Attempt: {attempt + 1} of {max_retries} -----\"\r\n                    )\r\n                    pipeline_start_time = datetime.now(timezone.utc)\r\n\r\n                    md = load_metadata(spark, sf_options, pid, cfg)\r\n                    logger.info(\r\n                        \"Loaded Metadata:\\n\" + json.dumps(md, indent=2, default=str)\r\n                    )\r\n\r\n                    layer = md[\"pipeline\"][\"LAYER\"].lower()\r\n                    logger.info(f\"Pipeline {pid} is a {layer} pipeline.\")\r\n\r\n                    errors = validate_all_required(\r\n                        spark, sf_options, pid, cfg\r\n                    )\r\n                    if errors:\r\n                        pipeline_end_time = datetime.now(timezone.utc)\r\n                        status_id, status_name = get_status_by_name(\r\n                            status_df, \"PIPELINE_FAILURE\", default_id=2\r\n                        )\r\n                        for emsg in errors:\r\n                            log_data_ingestion(\r\n                                spark,\r\n                                sf_options,\r\n                                master_log_id=master_log_id,\r\n                                status_message_id=status_id,\r\n                                pipeline_id=pid,\r\n                                layer=layer,\r\n                                trigger_type=\"SCHEDULED\",\r\n                                pipeline_start_time=pipeline_start_time,\r\n                                pipeline_end_time=pipeline_end_time,\r\n                                pipeline_status=\"PIPELINE_FAILURE\",\r\n                                errortype=status_name,\r\n                                error_message=emsg,\r\n                                error_severity=\"ERROR\",\r\n                                error_datetime=pipeline_end_time,\r\n                                source_count=0,\r\n                                destination_count=0,\r\n                                inserted_count=0,\r\n                                updated_count=0,\r\n                                deleted_count=0,\r\n                                log_table_name=LOG_TABLE,\r\n                            )\r\n                        any_failures = True\r\n                        attempt = max_retries\r\n                        break\r\n\r\n                    if layer == \"bronze\":\r\n                        source_df, dedupe_df, unique_keys = copy_parse_dedupe(\r\n                            spark, md, s3_staging_dir=None\r\n                        )\r\n                        logger.info(f\"Unique Keys: {unique_keys}\")\r\n                        dest_tbl, before_count = load_bronze(\r\n                            spark,\r\n                            sf_options,\r\n                            snowflake_secret,\r\n                            md,\r\n                            dedupe_df,\r\n                            unique_keys,\r\n                            stage_suffix=STAGE_TABLE_SUFFIX,\r\n                        )\r\n                        logger.info(\r\n                            f\"Bronze Layer ETL complete for PIPELINEID: {pid}\"\r\n                        )\r\n                    else:\r\n                        logger.error(\r\n                            f\"Unsupported layer '{layer}' for pipeline {pid}\"\r\n                        )\r\n                        raise Exception(\r\n                            f\"Unsupported layer '{layer}' for pipeline {pid}\"\r\n                        )\r\n\r\n                    try:\r\n                        after_count = spark_snowflake_retry(\r\n                            lambda: spark.read.format(\"snowflake\")\r\n                            .options(**sf_options)\r\n                            .option(\r\n                                \"query\", f\"SELECT COUNT(1) AS COUNT FROM {dest_tbl}\"\r\n                            )\r\n                            .load()\r\n                            .collect()[0][\"COUNT\"],\r\n                            action_desc=f\"get row count {dest_tbl} (after)\",\r\n                        )\r\n                        logger.info(\r\n                            f\"Fetched after_count from {dest_tbl}: {after_count}\"\r\n                        )\r\n                    except Exception as e:\r\n                        after_count = 0\r\n                        logger.warning(\r\n                            f\"Could not fetch after_count from Snowflake for {dest_tbl}, Error: {str(e)}\"\r\n                        )\r\n\r\n                    pipeline_end_time = datetime.now(timezone.utc)\r\n\r\n                    log_data_ingestion(\r\n                        spark,\r\n                        sf_options,\r\n                        master_log_id=master_log_id,\r\n                        status_message_id=PIPELINE_SUCCESS_ID,\r\n                        pipeline_id=pid,\r\n                        layer=layer,\r\n                        trigger_type=\"SCHEDULED\",\r\n                        pipeline_start_time=pipeline_start_time,\r\n                        pipeline_end_time=pipeline_end_time,\r\n                        pipeline_status=get_status_message_name_by_id(\r\n                            status_df, PIPELINE_SUCCESS_ID\r\n                        ),\r\n                        errortype=None,\r\n                        error_message=None,\r\n                        error_severity=None,\r\n                        error_datetime=None,\r\n                        source_count=source_df.count()\r\n                        if layer == \"bronze\"\r\n                        else 0,\r\n                        destination_count=after_count\r\n                        if layer == \"bronze\"\r\n                        else 0,\r\n                        inserted_count=max(0, after_count - before_count),\r\n                        updated_count=0,\r\n                        deleted_count=0,\r\n                        log_table_name=LOG_TABLE,\r\n                    )\r\n                    logger.info(\r\n                        f\"Logged data ingestion success for Pipeline {pid}.\"\r\n                    )\r\n\r\n                    update_cron_last_run(\r\n                        snowflake_secret, cron_id, table_name=CRON_TABLE\r\n                    )\r\n\r\n                    insert_global_config(\r\n                        master_log_id=master_log_id,\r\n                        md=md,\r\n                        job_id=glueRunId + \"_\" + master_log_id,\r\n                        start_ts=pipeline_start_time,\r\n                        end_ts=pipeline_end_time,\r\n                        record_count=before_count,\r\n                        status=\"C\",\r\n                        token=token,\r\n                    )\r\n\r\n                    break\r\n\r\n                except Exception as e:\r\n                    attempt += 1\r\n                    logger.error(\r\n                        f\"Pipeline {pid} attempt {attempt} failed: {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n                    )\r\n\r\n                    if attempt < max_retries:\r\n                        logger.info(\r\n                            f\"Retrying pipeline {pid} after {retrydelayseconds} seconds...\"\r\n                        )\r\n                        time.sleep(retrydelayseconds)\r\n                    else:\r\n                        any_failures = True\r\n                        pipeline_end_time = datetime.now(timezone.utc)\r\n                        status_id, status_name = classify_error(\r\n                            status_df, str(e)\r\n                        )\r\n\r\n                        log_data_ingestion(\r\n                            spark,\r\n                            sf_options,\r\n                            master_log_id=master_log_id,\r\n                            status_message_id=status_id,\r\n                            pipeline_id=pid,\r\n                            layer=layer,\r\n                            trigger_type=\"SCHEDULED\",\r\n                            pipeline_start_time=pipeline_start_time,\r\n                            pipeline_end_time=pipeline_end_time,\r\n                            pipeline_status=\"PIPELINE_FAILURE\",\r\n                            errortype=status_name,\r\n                            error_message=str(e),\r\n                            error_severity=\"ERROR\",\r\n                            error_datetime=pipeline_end_time,\r\n                            source_count=0,\r\n                            destination_count=0,\r\n                            inserted_count=0,\r\n                            updated_count=0,\r\n                            deleted_count=0,\r\n                            log_table_name=LOG_TABLE,\r\n                        )\r\n\r\n                        logger.error(\r\n                            f\"Pipeline {pid} failed after {max_retries} attempts. Classified as {status_name} (ID={status_id}).\"\r\n                        )\r\n\r\n        if any_failures:\r\n            master_status_name = get_status_message_name_by_id(\r\n                status_df, MASTER_FAILURE_ID\r\n            )\r\n            finalize_master_log(\r\n                snowflake_secret,\r\n                master_log_id,\r\n                master_status_name,\r\n                None,\r\n                table_name=MASTERLOG_TABLE,\r\n            )\r\n        else:\r\n            master_status_name = get_status_message_name_by_id(\r\n                status_df, MASTER_SUCCESS_ID\r\n            )\r\n            finalize_master_log(\r\n                snowflake_secret,\r\n                master_log_id,\r\n                master_status_name,\r\n                None,\r\n                table_name=MASTERLOG_TABLE,\r\n            )\r\n\r\n        master_message_content = get_statusmessage_content_by_name(\r\n            status_df, master_status_name\r\n        )\r\n        if master_message_content is not None:\r\n            safe_message = master_message_content.replace(\"'\", \"''\")\r\n            update_sql = (\r\n                f\"UPDATE {MASTERLOG_TABLE} SET MESSAGE = '{safe_message}' \"\r\n                f\"WHERE MASTERLOGID = '{master_log_id}'\"\r\n            )\r\n            execute_snowflake_sql(snowflake_secret, update_sql)\r\n\r\n    except Exception as e:\r\n        logger.error(\r\n            f\"Unexpected error occurred in main(): {str(e)}\\nTraceback:\\n{traceback.format_exc()}\"\r\n        )\r\n        raise\r\n\r\n    logger.info(\"===== sf_de_framework ETL Job Finished =====\")\r\n\r\n\r\n# ===== SCRIPT ENTRY POINT =====\r\nmain()\r\n"
}